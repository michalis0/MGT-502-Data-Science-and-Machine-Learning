{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords') # uncomment if stopwords is not up-to-date\n",
    "# nltk.download('punkt')     # uncomment if punkt is not up-to-date\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ML import\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA             # PCA\n",
    "from sklearn.manifold import TSNE                 # t-SNE\n",
    "from sklearn.manifold import MDS                  # Multidimensional scaling\n",
    "from sklearn.manifold import Isomap               # Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "<img src='https://media.tenor.com/fh89_ZNYrnwAAAAC/flat-earth-dinosaurs.gif' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Umsz9VL9Ia0a"
   },
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on Dimensionality Reduction. After presenting the main concepts, you will be introduced to the techniques to implement dimensionality reduction techniques, e.g., PCA, t-SNE, multidimensional scaling, Isomap. Finally, we'll explore a few applications to demonstrate the usefulness of dimensionality reduction for data visualization and classification tasks.  \n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [Background](#Background)\n",
    "    - [Principal Component Analysis](#Principal-Component-Analysis)\n",
    "        - [Finding principal components](#Finding-principal-components)\n",
    "        - [How many dimensions?](#How-many-dimensions?)\n",
    "        - [Practical issues of PCA](#Practical-issues-of-PCA)\n",
    "        - [Further reading](#Further-reading)\n",
    "    - [t-SNE](#t-SNE)\n",
    "        - [How does t-SNE work?](#How-does-t-SNE-work?)\n",
    "        - [Practical issues of t-SNE](#Practical-issues-of-t-SNE)\n",
    "        - [References and further reading](#References-and-further-reading)\n",
    "- [Implementation](#Implementation)\n",
    "    - [PCA](#PCA)\n",
    "    - [t-SNE and other Manifold Learning techniques](#t-SNE-and-other-Manifold-Learning-techniques)\n",
    "    - [Your turn!](#Your-turn!)\n",
    "- [Applications](#Applications)\n",
    "    - [PCA for visualization: Palmer penguins](#PCA-for-visualization:-Palmer-penguins)\n",
    "    - [SMS Classification: with vs. without PCA](#SMS-Classification:-with-vs.-without-PCA)\n",
    "        - [Discover dataset](#Discover-dataset)\n",
    "        - [Data preparation and cleaning](#Data-preparation-and-cleaning)\n",
    "        - [Dimensionality reduction and Classification](#Dimensionality-reduction-and-Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data.\n",
    "\n",
    "Dimensionality reduction is useful to:\n",
    "- Visualize data\n",
    "- Deal with the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "- Removing noise, leading to better prediction and classification accuracy\n",
    "- Reduce time complexity (faster to train algorithms with less features)\n",
    "- Compress data (less space needed)\n",
    "\n",
    "We generally distinguish between two sets of techniques:\n",
    "- [Feature selection](https://en.wikipedia.org/wiki/Feature_selection), which consists in selecting a subset of relevant features, using, for instance:\n",
    "    - domain knowledge ,\n",
    "    - information gain, e.g., in decision trees at each node we select the feature that maximizes the information gained,\n",
    "    - properties of your features, e.g., correlation, with the assumption that the prediction/classification performance is improved when features are highly correlated with the outcome, yet uncorrelated to each others,\n",
    "    - regularization techniques (LASSO, Ridge, Elastic Net)\n",
    "- [Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction), which consists in transforming our original features into new features that should be informative and non-redundant.\n",
    "\n",
    "This notebook focuses on feature extraction. We detail below a few techniques, namely Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "[Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) performs a linear combination of our original features into new and linearly-uncorrelated features, called **principal components**. The principal components are constructed such that:\n",
    "- the first principal component is the direction of greatest variability in the data, i.e., it has the largest possible [variance](https://en.wikipedia.org/wiki/Variance);\n",
    "- each succeeding component in turn has the highest variance possible, under the constraint that it is [orthogonal](https://en.wikipedia.org/wiki/Orthogonal) to the preceding components.\n",
    "\n",
    "The orthogonality constraint allows to build linearly-uncorrelated features. But why are we interested in the direction of greatest variability? Well, consider the figure below, the direction of the first principal component (V1) has the largest spread (variance). It is also the direction that distorts the least our observations when performing a projection. In other words, the direction of the largest variance is also the direction that minimizes the distances between the original points and their projections. In this sense, it is the most informative direction.  \n",
    "\n",
    "<img src='http://lazyprogrammer.me/wp-content/uploads/2015/11/PCA.jpg' width=\"400\">\n",
    "\n",
    "Source: [Tutorial: Principal Components Analysis (PCA)](https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/), Lazy Programmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding principal components\n",
    "\n",
    "How do we find the direction of largest variance? Let's formalize our problem!\n",
    "\n",
    "Say we have a matrix of $p$ features $\\boldsymbol{X}=[\\boldsymbol{x_1}$, ... , $\\boldsymbol{x_p}]$, each feature having $n$ observations $\\boldsymbol{x_j}=(x_{1j}, x_{2j}, ..., x_{nj})$. We assume that the mean of each feature is zero; otherwise the first step is to center the data at zero by subtracting the mean of each feature.\n",
    "\n",
    "Each principal component is a linear combination of our features, i.e., it can be represented by a vector of dimension $p$: $\\boldsymbol{\\phi_k} = (\\phi_{k,1}, \\phi_{k,2}, ..., \\phi_{k,p})$, where $\\phi_{k,j}$ are the weights associated with each feature. We impose that $\\boldsymbol{\\phi_k}$ is a unit vector, i.e., $|| \\boldsymbol{\\phi_k} ||=\\sum_j (\\phi_{k,j})^2=1$. Indeed, there exists an infinity of vectors that have the same direction, so we need to select one, and the unit vector leads to some interesting properties that we will discover below.\n",
    "\n",
    "For each observation $i = 1,...,n$ and each principal component $k$, we define a principal component *score* $z_{k,i}$:\n",
    "\n",
    "$$ z_{k,i} = \\phi_{k,1} x_{i1} + \\phi_{k,2} x_{i2} + ... + \\phi_{k,p} x_{ip} = \\sum_{j=1}^p \\phi_{k,j} x_{ij} $$\n",
    "\n",
    "Note that $z_{k,i}$ is the scalar [projection](https://en.wikipedia.org/wiki/Vector_projection) of observation $i$ on the principal component $\\boldsymbol{\\phi_k}$.  \n",
    "\n",
    "The first principal component is the direction of the largest variance. More explicitly, our problem consists in finding the weights $(\\phi_{1,1}, \\phi_{1,2}, ..., \\phi_{1,p})$ that maximizes the variance of the projections $\\boldsymbol{z_1}=(z_{1,1}, z_{1,2},..., z_{1,n})$, such that $\\boldsymbol{\\phi_1}$ is a unit vector. Note that since the mean of each feature is zero, the mean of $\\boldsymbol{z_1}$ is also zero. Hence, we need to solve the following problem:\n",
    "\n",
    "$$\\boldsymbol{\\phi_1} = \\arg \\max_{|| \\boldsymbol{\\phi_1} || =1} \\left\\{  \\frac{1}{n} \\sum_{i=1}^n (z_{1,i})^2 \\right \\} =  \\arg \\max_{|| \\boldsymbol{\\phi_1} || =1} \\left\\{  \\frac{1}{n} \\sum_{i=1}^n \\left(\\sum_{j=1}^p \\phi_{1,j} x_{ij} \\right)^2 \\right \\} $$\n",
    "\n",
    "Once $\\boldsymbol{\\phi_1}$ is found, we can compute for each observation $i$ the principal component score $z_{1,i}$, and $\\boldsymbol{z_1}$ is our new feature. \n",
    "\n",
    "We proceed similarly to find the other components: the $k^{th}$ component can then be found by subtracting the first $k$ − 1 principal components from $\\mathbf{X}$ and finding the weights that maximize the variance of the new projections.\n",
    "\n",
    "However, in practice, the principal components are computed differently. Let's take a step back and reconsider our optimization problem above. When solving it, by adding a Lagrangian multiplier $\\lambda_1$ and taking the derivatives with respect to each weight $\\phi_{1,j}$, we obtain, with some reshuffling:\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} \\boldsymbol{\\phi_1}  = \\lambda_1 \\boldsymbol{\\phi_1} $$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ is the covariance matrix of our features. We can get similar equations for all principal components $\\boldsymbol{\\Sigma} \\boldsymbol{\\phi_k}  = \\lambda_k \\boldsymbol{\\phi_k}$.\n",
    "\n",
    "If you are familiar with linear algebra, you will have recognized this equation: it means that the ***principal components are the [eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the covariance matrix of our features***! \n",
    "\n",
    "Not only that, we can show that, because $\\boldsymbol{\\phi_k}$ are unit vectors, the ***eigenvalues*** ($\\lambda_k$) are equal to the ***variance*** of the projections. Hence, the first principal component is the eigenvector of the covariance matrix with the largest eigenvalues, and so on.\n",
    "\n",
    "Consequently, we can compute the principal components thanks to [eigenvalue decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) of the covariance matrix or the more general [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)(SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many dimensions?\n",
    "\n",
    "We want to select $m$ principal components such that we (vastly) reduce our feature space ($m<<p$) while explaining the most variance (\"information\"). There are several techniques, such as: \n",
    "- using a [scree plot](https://en.wikipedia.org/wiki/Scree_plot): rank principal components by the amount of variance they capture in the original dataset, i.e., from largest to smallest eigenvalue, and look for the elbow of the curve\n",
    "- picking the first $m$ eigenvectors that explain a certain percentage of the total variance, e..g.90% or 95%: $\\frac{\\sum_{k=1}^m \\lambda_k}{\\sum_{k=1}^p \\lambda_k} \\geq 0.9$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical issues of PCA\n",
    "\n",
    "Even though PCA is a very nice technique building on the geometry of our data, there are a few practical considerations to keep in mind before using it:\n",
    "- since the covariance is extremely sensitive to large values, it is best to normalize your features to zero mean and unit variance before performing PCA,\n",
    "- PCA is a linear transformation, i.e., it assumes that the direction of greater variability is linear, which is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further reading\n",
    "\n",
    "To further deepen your understanding of PCA, you can refer to the following materials:\n",
    "- [Principal Component Analysis](https://www.youtube.com/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM) lecture by Victor Lavrenko (playlist of short videos), which includes the proofs of why the eigenvectors are the principal components and the eigenvalues the variance of the projections\n",
    "- [Everything you did and didn't know about PCA](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/), blog post by Alex Williams\n",
    "- [Eigenvectors and eigenvalues](https://www.youtube.com/watch?v=PFDu9oVAE-g), video by 3Blue1Brown, for a visual understanding of eigenvectors and eigenvalues\n",
    "- [A geometric interpretation of the covariance matrix](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/), blog post by Vincent Spruyt, to explore the geometric properties of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "Unlike PCA, [t-distributed stochastic neighbor embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t-SNE) is a [nonlinear dimensionality reduction](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) technique. The method attempts to learn a low-dimensional representation of the data that preserves the local structure of the data. It attempts to preserve pairwise distance and \"cluster\" information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does t-SNE work?\n",
    "\n",
    "t-SNE maps the data points from our initial high-dimensional space to a low-dimensional space in such a way that similar data points are close to each other in the low-dimensional space. The method involves two main steps:\n",
    "1. we compute the probability of similarity between each pair of data points in the high-dimensional space\n",
    "2. we compute the probability of similarity between each pair of data points in the low-dimensional space\n",
    "The objective is to minimize the difference between the two similarity probabilities to find the best mapping between the high-dimensional data and the low-dimensional space.\n",
    "\n",
    "Let's formalize this.We have a matrix of $p$ features $\\boldsymbol{X}$, each feature having $n$ observations. We call $\\boldsymbol{x_{(i)}}=(X_{i1}, X_{i2}, ..., X_{ip})$ the $i^{th}$ observation. Note that in the previous section $\\boldsymbol{x_{j}}$ was referring to feature $j$, i.e., a column of the matrix feature, while $\\boldsymbol{x_{(i)}}$ refers to observation $i$, i.e., a row of the matrix feature.\n",
    "\n",
    "The first step is to compute the similarity between each pair of data points. This similarity of datapoint $\\boldsymbol{x_{(a)}}$ to datapoint $\\boldsymbol{x_{(b)}}$ is defined to be the conditional probability $p_{b|a}$ that $\\boldsymbol{x_{(a)}}$ would pick $\\boldsymbol{x_{(b)}}$ as its neighbor, if the neighbors were picked in proportion to their probability density under a Gaussian centered at $\\boldsymbol{x_{(a)}}$:\n",
    "\n",
    "$$p_{b|a} = \\frac{\\exp(- ||\\boldsymbol{x_{(a)}} - \\boldsymbol{x_{(b)}}||^2 / 2 \\sigma_a^2)}{\\sum_{i \\neq a} \\exp(- ||\\boldsymbol{x_{(a)}} - \\boldsymbol{x_{(i)}}||^2 / 2 \\sigma_a^2)}$$\n",
    "\n",
    "We set $p_{a|a}=0$. The variance $\\sigma_a^2$ is set such that the entropy of the conditional distribution equals a predefined entropy, specified by a parameter called *perplexity*: $Perp = 2^{-\\sum_i p_{i|a} \\log_2 p_{i|a}}$. The perplexity can be interpreted as the number of neighbors for our central point. As a result, the variance is adjusted to the density of the data: smaller values of $\\sigma_a^2$ are used in denser part of the data space. \n",
    "\n",
    "See the illustration in the figure below, at the top is our initial space and at the bottom the similarities to the red observation.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Afr8xsKrl6dwZ10Q.png' width=\"500\">\n",
    "\n",
    "Source: [t-SNE clearly explained](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a), by Kermal Erdem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the variances $\\sigma_a^2$ and $\\sigma_b^2$ differ, so do $p_{b|a}$ and $p_{a|b}$. We thus define:\n",
    "$$p_{ab} = \\frac{p_{a|b}+p_{b|a}}{2n} $$\n",
    "\n",
    "Ok, we have defined similarities between two observations in our original data space. Our objective is to map our initial p-dimensional data points $\\boldsymbol{x_{(i)}}$ to lower-dimensional points $\\boldsymbol{y_{(i)}}$, such that the similarities in the high-dimensional space match the ones in the low- dimensional space. Thus, next step is to define similarities, $q_{ab}$, in a low-dimensional space. We use a [Student t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) (hence the name of the algorithm):\n",
    "\n",
    "$$q_{ab}=\\frac{(1+||\\boldsymbol{y_{(a)}} - \\boldsymbol{y_{(b)}}||^2)^{-1}}{\\sum_k \\sum_{k \\neq l}(1+||\\boldsymbol{y_{(k)}} - \\boldsymbol{y_{(l)}}||^2)^{-1}}$$\n",
    "\n",
    "Laurens Van der Maaten and Geoffrey Hinton, who introduced this distribution, discuss the rationale behind this assumption in their paper [Visualizing data using t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).\n",
    "\n",
    "Last step, we find the new data points $\\boldsymbol{y_{(i)}}$ by minimizing the distance between the two similarity distributions. We use the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):\n",
    "\n",
    "$$\\sum_{a \\neq b} p_{ab} \\log \\frac{p_{ab}}{q_{ab}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical issues of t-SNE\n",
    "\n",
    "t-SNE is effective in preserving the local structure of the data points, making it ideal for visualizing complex and non-linear data. As drawbacks, the method is:\n",
    "- computationally expensive\n",
    "- sensitive to initial conditions, e.g., perplexity. You can read [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/), by Martin Wattenberg, Fernanda Viégas and Ian Johnson, for a nice discussion and good practices\n",
    "- not guaranteed to preserve the global structure of data. For instance, t-SNE is not recommended for clustering analysis or outlier detection since it does not necessarily preserve densities or distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References and further reading\n",
    "\n",
    "- Van der Maaten, L., & Hinton, G. (2008). [Visualizing data using t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). *Journal of machine learning research*, 9(11).\n",
    "- [t-SNE clearly explained](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a), by Kemal Erdem\n",
    "- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/), by Martin Wattenberg, Fernanda Viégas and Ian Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3FNBRUHIa0b"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "We present in this section how to implement dimensionality reduction techniques using some synthetic data to better understand the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "We use the `PCA` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA))\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "```\n",
    "\n",
    "Ok, let's try with some synthetic data, in two dimensions to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate 2D points\n",
    "rng = np.random.RandomState(7)\n",
    "X_2D = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "\n",
    "## Plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_2D[:, 0], X_2D[:, 1])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `PCA()` specifying the number of components `n_components`. Here, we are in 2 dimensions, there are thus 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "# Create instance of PCA with the desired number of components\n",
    "pca_2D = PCA(n_components=2)\n",
    "# Fit: find principal components\n",
    "pca_2D.fit(X_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.fit()` we have learned our principal components, i.e., the eigenvectors of the covariance matrix, and the explained variance, i.e., the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print components = eigenvectors\n",
    "print('The eigenvectors are:')\n",
    "print(pca_2D.components_)\n",
    "\n",
    "# Print explained variance = eigenvalues\n",
    "print('The eigenvalues are:', pca_2D.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this. We use the components to define the direction of the vector, and the explained variance to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw vectors\n",
    "def draw_vector(v0, v1, color, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2.5,\n",
    "                    color = color,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "## Plot\n",
    "colors_vec = ['red','coral']\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_2D[:, 0], X_2D[:, 1])\n",
    "for length, vector, col in zip(pca_2D.explained_variance_, pca_2D.components_, colors_vec):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca_2D.mean_, pca_2D.mean_ - v, col)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red vector is the first principal component and the orange one the second principal component. As seen in the figure, they are orthogonal and the first principal component is the direction of largest variance.\n",
    "\n",
    "Let's now transform our points to reduce the dimension. In other words, we will project our points on the first principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "# Create instance of PCA with the desired number of components\n",
    "pca_1D = PCA(n_components=1)\n",
    "# Fit: find principal components\n",
    "pca_1D.fit(X_2D)\n",
    "# Transform our points: projection\n",
    "X_2D_pca = pca_1D.transform(X_2D)\n",
    "# Note: instead of .fit() and .transform(), we could directly use .fit_transform()\n",
    "\n",
    "print(\"Original shape:   \", X_2D.shape)\n",
    "print(\"Transformed shape:\", X_2D_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transformed our 2D points into 1D point. To transform the data back to its original dimension, we can use the method `.inverse_transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform projections to original dimension\n",
    "X_2D_new = pca_1D.inverse_transform(X_2D_pca)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_2D[:, 0], X_2D[:, 1], alpha = 0.3)\n",
    "plt.scatter(X_2D_new[:, 0], X_2D_new[:, 1], color = 'blue')\n",
    "v1 = pca_1D.components_[0] * 2.5 * np.sqrt(pca_1D.explained_variance_)\n",
    "draw_vector(pca_1D.mean_, pca_1D.mean_ -  v1, 'black')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As displayed in the figure, the 2D data points were projected on the first principal components. In other words, with PCA, the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality. The reduced-dimension dataset should preserve the overall relationship between the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the weakness of PCA is that it tends to be highly affected by outliers in the data. Many robust variants of PCA have been developed to overcome this limitation, with the idea to iteratively discard data points that are poorly described by the initial components. For instance, `SparsePCA` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html)) introduces a regularization term that serves to enforce sparsity of the components. \n",
    "\n",
    "Discover other options in sklearn [user guide](https://scikit-learn.org/stable/modules/decomposition.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE and other Manifold Learning techniques\n",
    "\n",
    "We have previously discussed that PCA assumes that the direction of greater variability is linear. What if it is not the case? For instance, consider the famous [Swiss roll](https://en.wikipedia.org/wiki/Swiss_roll) (which is not from Switzerland...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Swiss roll\n",
    "sr_points, sr_color = datasets.make_swiss_roll(n_samples=1500, random_state=0)\n",
    "\n",
    "## PCA\n",
    "sr_pca = PCA(n_components=2).fit_transform(sr_points)\n",
    "\n",
    "## Plot\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "# Swiss Roll\n",
    "ax = fig.add_subplot(1,2,1, projection=\"3d\")\n",
    "ax.scatter(sr_points[:, 0], sr_points[:, 1], sr_points[:, 2], c=sr_color, s=50, alpha=0.8)\n",
    "ax.set_title(\"Swiss Roll\")\n",
    "ax.view_init(azim=-60, elev=9)\n",
    "# Swiss Roll projected with PCA\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.scatter(sr_pca[:, 0], sr_pca[:, 1], c=sr_color)\n",
    "ax2.set_title(\"PCA Embedding of Swiss Roll\")\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened? The PCA \"cut\" the Swiss roll, but by doing so we lost some information.\n",
    "\n",
    "Let's try with manifold learning, i.e., nonlinear dimensionality reduction techniques:\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE), using the `TSNE` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)).\n",
    "    - as parameters, we specify the number of components (dimensions) `n_components` and the `perplexity`, which can be interpreted as the number of nearest neighbors.\n",
    "- [Multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling) (MDS) , using the `MDS` module ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html))\n",
    "    - MDS seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space\n",
    "    - as parameters, we specify the number of components (dimensions) `n_components` \n",
    "- [Isomap](https://en.wikipedia.org/wiki/Isomap), using the `Isomap` module ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html))\n",
    "    - Isomap seeks a lower-dimensional embedding which maintains [geodesic](https://en.wikipedia.org/wiki/Geodesic) distances between all points. It can be viewed as an extension of MDS, using geodesic distances instead of Euclidean distances. In short, the idea is to: \n",
    "        1) determine the k nearest neighbors of each point\n",
    "        2) construct a neighborhood graph by connecting each point to its nearest neighbors\n",
    "        3) compute the geodesic distances between points, defined as the shortest path\n",
    "        4) find the lower-dimensional embedding such that the (Euclidean) distances in the low-dimensional space are close to the geodesic distances \n",
    "    - as parameters, we specify the number of components (dimensions) `n_components` and the number of neighbors to each point `n_neighbors`\n",
    "\n",
    "Do not hesitate to check the [user guide](https://scikit-learn.org/stable/modules/manifold.html) for more details on each approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-SNE\n",
    "# Create instance of TSNE\n",
    "tsne_sr = TSNE(n_components=2, perplexity=40, random_state=0)\n",
    "# Fit and project points\n",
    "start = time.time()\n",
    "sr_tsne = tsne_sr.fit_transform(sr_points)\n",
    "end = time.time()\n",
    "print('Time to run t-SNE algorithm', end-start)\n",
    "\n",
    "## MDS\n",
    "# Create instance of MDS\n",
    "mds_sr = MDS(n_components=2, max_iter=50, random_state=0, normalized_stress = 'auto')\n",
    "# Fit and project points\n",
    "start = time.time()\n",
    "sr_mds = mds_sr.fit_transform(sr_points)\n",
    "end = time.time()\n",
    "print('Time to run MDS algorithm', end-start)\n",
    "\n",
    "## Isomap\n",
    "# Create instance of Isomap\n",
    "isomap_sr = Isomap(n_components=2, n_neighbors=10)\n",
    "# Fit and project points\n",
    "start = time.time()\n",
    "sr_isomap = isomap_sr.fit_transform(sr_points)\n",
    "end = time.time()\n",
    "print('Time to run Isomap algorithm', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap was much faster than t-SNE and MDS. But what about the quality of the projections? Let's visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot\n",
    "fig, axs = plt.subplots(1,3, figsize=(16, 4))\n",
    "axs[0].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n",
    "axs[0].set_title(\"t-SNE Embedding of Swiss Roll\")\n",
    "axs[1].scatter(sr_mds[:, 0], sr_mds[:, 1], c=sr_color)\n",
    "axs[1].set_title(\"MDS Embedding of Swiss Roll\")\n",
    "axs[2].scatter(sr_isomap[:, 0], sr_isomap[:, 1], c=sr_color)\n",
    "axs[2].set_title(\"Isomap Embedding of Swiss Roll\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE seems to preserve the general structure of the data, but, poorly represents the continuous nature of the Swiss roll. MDS improves a bit over PCA (more information is retained), but does not capture well the pattern of our manifold. On the other hand, Isomap perfectly unroll the Swiss roll!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Consider the following \"noisy plane\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plane function -> ax + by + cz = d + e\n",
    "rng = np.random.RandomState(7)\n",
    "\n",
    "def noisy_plane(a, b, c, d, noise, n):\n",
    "    \"\"\"Function returns n points belonging to a plane in 3D, with some added noise.\n",
    "       Equation of plane -> ax + by + cz = d\n",
    "    \"\"\"\n",
    "    # Generate n points\n",
    "    x = 100*rng.rand(n)\n",
    "    y = 100*rng.rand(n)\n",
    "    # Noise\n",
    "    e = rng.normal(0, noise, n)\n",
    "    # Calculate the third coordinate of the plane\n",
    "    z = (d - a*x - b*y + e)/c\n",
    "    # Make and return DataFrame\n",
    "    df = pd.DataFrame(np.array([x,y,z])).transpose()\n",
    "    df.columns = ['x', 'y', 'z']\n",
    "    return df\n",
    "\n",
    "# Number of points\n",
    "n_samples = 500\n",
    "\n",
    "# Plane with noise of 50\n",
    "df = noisy_plane(-2, -1, -3, 0, 25, n_samples)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(df['x'], df['y'], df['z'])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(azim=-45, elev=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement PCA with 2 components. Show (print) the components and the explained variance. Plot the projections of the points in a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement manifold learning techniques (e.g., t-SNE, Isomap, MDS) and compare the results to what you obtained with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following 3-dimensional S-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_points, S_color = datasets.make_s_curve(1500, random_state=0)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(\n",
    "    S_points[:, 0], S_points[:, 1], S_points[:, 2], c=S_color, s=50, alpha=0.8\n",
    ")\n",
    "ax.set_title(\"S-Curve\")\n",
    "ax.view_init(azim=-66, elev=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement various manifold learning techniques, playing with the parameters of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "In this section, we present a few applications of dimensionality reduction techniques with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for visualization: Palmer penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) dataset. It contains information about 3 types of Palmer penguins, namely Adelie, Gentoo and Chinstrap: \n",
    "\n",
    "<img src='https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png' width=\"500\">\n",
    "\n",
    "Source: Artwork by @allison_horst \n",
    "\n",
    "Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n",
    "\n",
    "Reference: Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. DOI: [10.5281/zenodo.3960218](https://allisonhorst.github.io/palmerpenguins/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "k_2Ke1KC2wR8",
    "outputId": "e7967ec9-5bb3-4b90-d09c-8537e47a9e16"
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "url_penguin = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/penguins.csv' \n",
    "\n",
    "df_peng = pd.read_csv(url_penguin).dropna().reset_index()\n",
    "df_peng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 numerical features ('bill length', 'bill depth', 'flipper length', and 'body mass'), so visualizing information about our penguins would require to select 2 or 3 features, or instead, to use dimensionality reduction! \n",
    "\n",
    "Let's check the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "dp7Etd1V2wNJ",
    "outputId": "8c2db678-eaaf-4b0a-8f2a-521baaccad32"
   },
   "outputs": [],
   "source": [
    "# Correlation\n",
    "X_peng = df_peng[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(X_peng.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gE3el6Z3aln"
   },
   "source": [
    "There are some high correlations, so PCA may be useful. Note that our features have very different scales. Hence, before applying PCA, each feature should be centered with unit variance. We use the `StandardScaler` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "_0PO9fh73rW4",
    "outputId": "ec57a550-ebf5-44dd-9687-eba0882ff0ac"
   },
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "X_peng_normalized = StandardScaler().fit_transform(X_peng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZMXhRT57p2K"
   },
   "source": [
    "We now apply PCA reducing our 4-dimensional space to 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with 2 components\n",
    "pca_peng = PCA(n_components = 2).fit(X_peng_normalized)\n",
    "X_peng_pca = pca_peng.transform(X_peng_normalized)\n",
    "\n",
    "# Visualize in dataframe\n",
    "df_pca_peng = pd.DataFrame(data = X_peng_pca[:,:2], columns = ['PC 1', 'PC 2'])\n",
    "df_pca_peng['species'] = df_peng['species']\n",
    "df_pca_peng['sex'] = df_peng['sex']\n",
    "df_pca_peng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'Adelie':'orange', 'Gentoo': 'teal', 'Chinstrap':'orchid'}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=df_pca_peng, x=\"PC 1\", y=\"PC 2\", \n",
    "                    hue=\"species\", palette = colors, style=\"sex\")\n",
    "plt.title('Visualization of Palmer Penguins with PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBxtlR066KBX"
   },
   "source": [
    "We can see a clear separation between Gentoo penguins and the other species. The distinction between male and female is also preserved. The main overlap is between female Chinstrap and male Adelie.\n",
    "\n",
    "Let's have a look at our eigenvector to check the magnitude of each original feature in the principal components. We'll create a dataframe and also add the percentage of explained variance by each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2XNVCQAEbEc",
    "outputId": "c5616589-c50b-4b7b-ab54-925828008164"
   },
   "outputs": [],
   "source": [
    "feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "# Principal components\n",
    "df_pca_peng_components = pd.DataFrame(pca_peng.components_, columns=feature_names, index=['PC 1', 'PC 2'])\n",
    "df_pca_peng_components['Explained variance [%]'] = pca_peng.explained_variance_ratio_\n",
    "df_pca_peng_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first principal component explains about 69% of the variance, and the second principal component explains about 19%, so 88% overall. Not bad! \n",
    "\n",
    "Finally, we can visualize the magnitude of each original feature in the principal components with a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "X-1DemmE9Ep3",
    "outputId": "035258c9-77dc-4cbc-def5-ab7f9518036b"
   },
   "outputs": [],
   "source": [
    "# Plot feature magnitude\n",
    "plt.figure(figsize=(6, 2))\n",
    "sns.heatmap(df_pca_peng_components[feature_names], annot = True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEZ_-I_oIa0c"
   },
   "source": [
    "### SMS Classification: with vs. without PCA\n",
    "\n",
    "We classify SMS as \"ham\" (legitimate) or \"spam\" comparing the performance of logistic regression and KNN with and without PCA. We use the [SMS Spam Collection dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_SDOdbNIa0c"
   },
   "source": [
    "#### Discover dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VGtlXQB0Ia0c",
    "outputId": "bdd2b96a-8bfe-4083-bf45-953c6d37ed39",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from GitHub\n",
    "url_spam = \"https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/spam.csv\"\n",
    "df_spam = pd.read_csv(url_spam, encoding='latin-1').drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1)\n",
    "df_spam = df_spam.rename(columns = {'v1':'label','v2':'message'})\n",
    "df_spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCZs9-8rHiB9"
   },
   "source": [
    "We want to predict the label based on the text of the message. Let's explore a bit our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYpC6Kt_Ia0c",
    "outputId": "6c03133f-3f23-41db-cfae-d0cbc44ec568"
   },
   "outputs": [],
   "source": [
    "df_spam.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "5p0rPuKGIa0d",
    "outputId": "800840a9-6f6a-405c-e805-1a467b3e81ca"
   },
   "outputs": [],
   "source": [
    "# Proportion of each class\n",
    "spam_freq = df_spam.label.value_counts(normalize=True)\n",
    "print(\"The proportion of legitimate message is: \", spam_freq[0])\n",
    "print(\"The proportion of spam message is: \", spam_freq[1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(3,4))\n",
    "sns.countplot(x=df_spam.label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxKa53-83IIP"
   },
   "source": [
    "#### Data preparation and cleaning\n",
    "\n",
    "The first step is, as always, to split our dataset between training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHqSuxtZ2bd2"
   },
   "outputs": [],
   "source": [
    "# Select variables\n",
    "X = df_spam.message\n",
    "y = df_spam.label\n",
    "\n",
    "# Split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEk4GwrcIa0d"
   },
   "source": [
    "Next, we clean the text. We use the `re` and `nltk` packages for cleaning ([NLTK Documentation](https://www.nltk.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lR5uZhkYIa0d",
    "outputId": "5091c1b0-ed74-4264-e852-aa9a3170d715"
   },
   "outputs": [],
   "source": [
    "# Define cleaning function\n",
    "def data_cleaner(sms):\n",
    "    \n",
    "    # Define stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Remove digits\n",
    "    sms = re.sub(r\"\\d+\",\"\", sms)\n",
    "    \n",
    "    # Lowercase\n",
    "    sms = sms.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    sms = re.sub(r\"[^\\w\\s\\d]\",\"\", sms)\n",
    "    \n",
    "    # Remove stop words\n",
    "    sms = sms.split()\n",
    "    sms = \" \".join([word for word in sms if not word in stop_words])\n",
    "    \n",
    "    # Tokenize\n",
    "    sms = word_tokenize(sms)\n",
    "    \n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    sms = [ps.stem(word) for word in sms]\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSgFrpp8IJlk"
   },
   "source": [
    "We use TF-IDF to vectorize the messages, using our previously-defined function as tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjmXYp4g1fbb",
    "outputId": "1262594f-c83c-47e8-9fd5-ccfef15c2939"
   },
   "outputs": [],
   "source": [
    "# Define vectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, tokenizer=data_cleaner, ngram_range=(1,1), min_df=3, max_df=0.9)\n",
    "\n",
    "# Fit and transform X_train and X_test\n",
    "X_train_vec = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_vec = tfidf.transform(X_test).toarray()\n",
    "print(\"The dimensions of our (vectorized) training set are: \",X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h3jN-yYIR00"
   },
   "source": [
    "As we can see, `X_train_vec` has a dimensionality of 1957."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0aWtx3eIa0d"
   },
   "source": [
    "#### Dimensionality reduction and Classification\n",
    "\n",
    "We will compare the running time and accuracy on the training and test sets of:\n",
    "- Logistic regression and KNN\n",
    "    - With PCA using 100 components and without\n",
    "        - With and without Standardization\n",
    "        \n",
    "We first define a function that uses as input a classification model and returns the running time and training and test set accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def spam_classification(model):\n",
    "    \"\"\"This function returns a dataframe with:\n",
    "        - the running time\n",
    "        - the training set accuracy\n",
    "        - the test set accuracy\n",
    "    of a classification model\"\"\"\n",
    "    \n",
    "    # Fit model\n",
    "    start = time.time()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    end = time.time()\n",
    "    \n",
    "    # Indicators\n",
    "    run_time = round(end-start, 4)\n",
    "    train_accuracy = round(model.score(X_train_vec, y_train), 4)\n",
    "    test_accuracy = round(pipe.score(X_test_vec, y_test), 4)\n",
    "    results = [run_time, train_accuracy, test_accuracy]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create instances of `PCA`, with 100 components for logistic regression and with 10 components for KNN, and of `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca_knn = PCA(n_components=10)\n",
    "\n",
    "# Define Scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our models and use the above function to train and test our algorithms, returning the results in a dataframe:\n",
    "- Logistic Regression, without Standardization, without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression, without Standardization, without PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('logistic reg', LogisticRegression())\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results = pd.DataFrame(data=results,\n",
    "                           index = ['Running time', 'Training accuracy', 'Test accuracy'],\n",
    "                           columns = ['Logistic Regression, without Standardization, without PCA:'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression, with Standardization, without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression, with Standardization, without PCA:\n",
    "pipe = Pipeline([('scaler', scaler),\n",
    "                 ('logistic reg', LogisticRegression())\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['Logistic Regression, with Standardization, without PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression, without Standardization, with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression, without Standardization, with PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('pca', pca),\n",
    "                 ('logistic reg', LogisticRegression()),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['Logistic Regression, without Standardization, with PCA '] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression, with Standardization, with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression, with Standardization, with PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', scaler),\n",
    "                 ('pca', pca),\n",
    "                 ('logistic reg', LogisticRegression(max_iter=3000)),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['Logistic Regression, with Standardization, with PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN, without Standardization, without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN, without Standardization, without PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('knn', KNeighborsClassifier(15)),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['KNN, without Standardization, without PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN, with Standardization, without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN, with Standardization, without PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', scaler),\n",
    "                 ('knn', KNeighborsClassifier(15)),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['KNN, with Standardization, without PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN, without Standardization, with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN, without Standardization, with PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('pca', pca_knn),\n",
    "                 ('knn', KNeighborsClassifier(15)),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['KNN, without Standardization, with PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN, with Standardization, with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN, with Standardization, with PCA:\n",
    "pipe = Pipeline([\n",
    "                 ('scaler', scaler),\n",
    "                 ('pca', pca_knn),\n",
    "                 ('knn', KNeighborsClassifier(15)),\n",
    "                 ])\n",
    "results = spam_classification(pipe)\n",
    "df_spam_results['KNN, with Standardization, with PCA'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "df_spam_results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB5NrbqpI7hN"
   },
   "source": [
    "We notice that:\n",
    "- for Logistic Regression, the best results are obtained with Standardization but without PCA. That being said, in all cases the accuracy is quite high.\n",
    "- for KNN, PCA increases (a lot) the accuracy but standardization does not\n",
    "\n",
    "Hence, PCA and standardization are useful tools that can help increase the accuracy, but not always..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
