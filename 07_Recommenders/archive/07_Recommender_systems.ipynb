{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import norm\n",
    "from collections import defaultdict\n",
    "from inspect import signature\n",
    "import sklearn.preprocessing as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "# Import from surprise library\n",
    "from surprise import Dataset\n",
    "from surprise import get_dataset_dir\n",
    "from surprise import KNNBasic, KNNWithMeans, SVD\n",
    "from surprise.model_selection import train_test_split \n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "# Set some formatting options\n",
    "plt.style.use('dark_background')\n",
    "np.set_printoptions(threshold=500, precision=4)\n",
    "pd.options.display.max_seq_items = 20\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12cW8RItmOpj"
   },
   "source": [
    "# Recommender Systems\n",
    "\n",
    "<img src='https://imgs.xkcd.com/comics/star_ratings.png' width=\"300\">\n",
    "\n",
    "Source: [xkcd 1908](https://xkcd.com/1098/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on recommender systems. A [recommender system](https://en.wikipedia.org/wiki/Recommender_system) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. They are primarily used in commercial applications.\n",
    "\n",
    "In this notebook, we will see how to implement a recommender from scratch before using the surprise library:\n",
    "- [Implementing User-User Collaborative Filtering from scratch](#Implementing-User-User-Collaborative-Filtering-from-scratch)\n",
    "    - [Load data](#Load-data)\n",
    "    - [Data visualization](#Data-visualization)\n",
    "    - [Preprocessing](#Preprocessing)\n",
    "    - [Create the Ratings Matrix](#Create-the-Ratings-Matrix)\n",
    "    - [User Average Ratings](#User-Average-Ratings)\n",
    "    - [Compute User-User Similarity](#Compute-User-User-Similarity) \n",
    "    - [User to all Users Similarities](#User-to-all-Users-Similarities)\n",
    "    - [Create User Neighborhood](#Create-User-Neighborhood) \n",
    "    - [Predict a Rating](#Predict-a-Rating) \n",
    "    - [Recommendation](#Recommendation)\n",
    "- [Using the Surprise library](#Using-the-Surprise-library)\n",
    "    - [Building a recommender system](#Building-a-recommender-system)\n",
    "    - [Visualizing the recommendations](#Visualizing-the-recommendations)\n",
    "    - [Precision and Recall at rank k](#Precision-and-Recall-at-rank-k)\n",
    "    - [Precision-recall curve](#Precision-recall-curve)\n",
    "    - [Cross-validation](#Cross-validation)\n",
    "    - [Tuning hyper-parameters](#Tuning-hyper-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvwKN6fqmMZ8"
   },
   "source": [
    "## Implementing User-User Collaborative Filtering from scratch\n",
    "\n",
    "Our goal is to implement User-User Collaborative Filtering from scratch, i.e., by only using `numpy` and `scipy` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmljqGT8nAAF"
   },
   "source": [
    "### Load data\n",
    "\n",
    "We will use the MovieLens dataset, which contains movie ratings collected from the MovieLens website by the [GroupLens](https://grouplens.org/) research lab, and more specifically the [small dataset for education and development](https://grouplens.org/datasets/movielens/).\n",
    "\n",
    "Source: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. *ACM Transactions on Interactive Intelligent Systems (TiiS)* 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVjGJMVXmMZ-"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings_raw = pd.read_csv('https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv('https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/ml-latest-small/movies.csv')\n",
    "links = pd.read_csv('https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/ml-latest-small/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikvODPGbmMZ-"
   },
   "source": [
    "The \"ratings.csv\" dataset contains the ratings given to movies by users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "6mWQnnUcmMZ-",
    "outputId": "5f39b401-6bc2-4ac8-85cc-a6e2d4fc6f6f"
   },
   "outputs": [],
   "source": [
    "display(ratings_raw.head())\n",
    "print(\"Distinct users:\", len(ratings_raw.userId.unique()))\n",
    "print(\"Distinct movies:\", len(ratings_raw.movieId.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the movies are identified by an Id number. The \"movies.csv\" file gives us the title corresponding to a given Id number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the \"links.csv\" file gives the identifiers that can be used to link to other sources of movie data, such as [IMDb](https://www.imdb.com/) and The Movie Database ([TMDb](https://www.themoviedb.org/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(links.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1oYI3WxmMZ-"
   },
   "source": [
    "### Data visualization\n",
    "\n",
    "Lets visualize some of the movie data. We will use [tmdbsimple](https://pypi.org/project/tmdbsimple/) which is a wrapper, written in Python, for The Movie Database (TMDb) API v3. You can install tmdbsimple with the following line of code in your terminal:\n",
    "\n",
    "```python\n",
    "pip install tmdbsimple\n",
    "``` \n",
    "\n",
    "*Note*: You could also install a module directly from a Jupyter Notebook by using `!pip install`. The exclamation mark `!` allows you to run shell commands from inside a Jupyter Notebook code cell. However, you need to be careful when doing so since this could lead to some errors. For further information, see this discussion on how to [Install Python Packages from a Jupyter Notebook](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also directly import a module from GitHub using the [httpimport](https://pypi.org/project/httpimport/) module. As before, you can install it with the following line of code:\n",
    "\n",
    "```python\n",
    "pip install httpimport\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elTxW5fOn94T"
   },
   "source": [
    "We import the \"TMDB_class\" module, which contains a class we need called `TMDB`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T97y7f2YuQgB",
    "outputId": "582128fd-1735-4543-db4e-836f1335bb20"
   },
   "outputs": [],
   "source": [
    "# Import module from GitHub\n",
    "import httpimport\n",
    "with httpimport.remote_repo('https://github.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/tree/main/07_Recommenders/TMDB_class'):\n",
    "    import TMDB_class\n",
    "\n",
    "TMDB_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two functions to help us in our exploration. The first function returns the HTML format of an image URL. The second function is using the first one to show the movie posters of the top rated movies for a given user identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujlt6Z-mmMZ-"
   },
   "outputs": [],
   "source": [
    "# Image url in html format\n",
    "def make_html(image_url):\n",
    "     return '<img src=\"{}\" style=\"display:inline;margin:1px\" width=\"100\"/>'\\\n",
    "            .format(image_url)\n",
    "\n",
    "# Display movie posters from userID    \n",
    "def show_movies_for_user(userId, verbose=False, show_all=False):\n",
    "    \"\"\"Retrieve posters of top rated movies for userId.\n",
    "    \"\"\"\n",
    "    html = ''\n",
    "    max_movies = 10\n",
    "    i=0\n",
    "    \n",
    "    # Extract all movies watched by user, sorted by rating\n",
    "    user_movies = ratings_raw[ratings_raw.userId == userId].sort_values(\"rating\", ascending=False)\n",
    "    # Print number of watched movies\n",
    "    print(f'User {userId} watched {len(user_movies)} movies.') \n",
    "    \n",
    "    # Loop through the movies to retrieve posters\n",
    "    for index, row in user_movies.iterrows():\n",
    "        movieId = row[\"movieId\"]                         # Extract MovieLens movie Id                 \n",
    "        movie_item = links[links.movieId == movieId]     # Extract row corresponding to movieId from links dataset\n",
    "        tmdbId = movie_item[\"tmdbId\"].item()             # Extract TMDb movie Id\n",
    "        # Print Id of movies in MovieLens and in TMDb is verbose=True\n",
    "        if verbose:\n",
    "            print(movieId, tmdbId)\n",
    "        # Retrieve TMDb movie poster url    \n",
    "        if np.isnan(tmdbId):\n",
    "            url = None\n",
    "        else:\n",
    "            url = TMDB_class.TMDB().get_poster_path_by_id(int(tmdbId)) # use imported module\n",
    "        html = html + make_html(url)                                   # convert image url to html\n",
    "        i +=1\n",
    "        # Only display top 10 movies if show_all = False\n",
    "        if ~show_all and (i == max_movies):\n",
    "                break\n",
    "        \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 10 movies watched by user 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "gkYybOWomMZ_",
    "outputId": "a558dc48-a0cd-45df-813b-a7b2db7087cb"
   },
   "outputs": [],
   "source": [
    "show_movies_for_user(6, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the Lion King is among the top. Let's display the ratings and general information on these movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "tVd_Cg_-E7oW",
    "outputId": "d41b0c05-d2a8-429a-d64d-65382ed8ab2e"
   },
   "outputs": [],
   "source": [
    "(ratings_raw[ratings_raw.userId == 6]                       # Extract movies ratings for user ID = 6\n",
    "            .sort_values(\"rating\", ascending=False)         # Sort by rating\n",
    "            .head(10)                                       # Extract top 10 movies\n",
    "            .set_index('movieId')                           # Reset index by movie Id\n",
    "            .join(movies.set_index('movieId'), on='movieId') # Join with movies dataframe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define additional functions to pursue our analysis. The first one returns the name of a movie using the associated identifier. The second one plots a histogram of movies genres watched by one user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbnQLvxDmMZ_"
   },
   "outputs": [],
   "source": [
    "def get_name_for_movie_id(movie_id):\n",
    "    \"\"\"Returns the name of a movie_id (based on ratings_raw i.e. original id)\"\"\"    \n",
    "    try: \n",
    "        movie_name = movies[movies.movieId == movie_id][\"title\"].item()\n",
    "    except KeyError:\n",
    "        movie_name = None\n",
    "    return movie_name\n",
    "    \n",
    "    \n",
    "def show_genres_histogram_for_user(user_id):\n",
    "    \"\"\"Create histogram of movies genres user_id has watched.\n",
    "    \"\"\"\n",
    "    # Extract movie Ids\n",
    "    user_movies = ratings_raw[ratings_raw.userId == user_id][\"movieId\"]\n",
    "    # Print number of watch movies\n",
    "    print(f'User {user_id} watched {len(user_movies)} movies.') \n",
    "    # Extract dataframe with title and genre of movies watched\n",
    "    user_movies_with_genre = movies[movies.movieId.isin(user_movies)]\n",
    "    display(user_movies_with_genre)\n",
    "    # List of all genres of movies watched (include duplicates)\n",
    "    genres_list = []\n",
    "    for index, row in user_movies_with_genre.iterrows():\n",
    "        genres_list += row[\"genres\"].split('|')\n",
    "    # Dataframe of all genres\n",
    "    df_genres = pd.DataFrame(genres_list, columns=['Genres'])\n",
    "    # Histogram of movies genres\n",
    "    df_genres.groupby('Genres').size().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "eWzVyYx4Z82u",
    "outputId": "c066a02f-d415-4ab6-f14e-25999ccf5b9c"
   },
   "outputs": [],
   "source": [
    "get_name_for_movie_id(364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "id": "e7QVi7s5mMZ_",
    "outputId": "f0130bf7-dcf8-4b65-9952-fc6b2e7f6d64"
   },
   "outputs": [],
   "source": [
    "show_genres_histogram_for_user(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg6HAA8MmMZ_"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "To create our ratings matrix, we need to make sure that movies and users have consecutive indexes starting from 0. We thus create two dictionaries mapping: 1) the movie identifiers (movieId) to movies indices (movieIDX); 2) the users identifiers (userId) to users indices (userIDX). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "OU2AlsSNmMZ_",
    "outputId": "fbccb93f-86f1-4405-bfa4-5a23403a2003"
   },
   "outputs": [],
   "source": [
    "# Extract and sort unique movies and users\n",
    "movieIds = ratings_raw.movieId.unique()\n",
    "movieIds.sort()\n",
    "userIds = ratings_raw.userId.unique()\n",
    "userIds.sort()\n",
    "\n",
    "# Size\n",
    "m = movieIds.size\n",
    "n = userIds.size\n",
    "numRatings = len(ratings_raw)\n",
    "print(f'There are {n} users, {m} movies, and {numRatings} ratings.')\n",
    "\n",
    "## Movies and users should have consecutive indexes starting from 0\n",
    "# Dictionaries to convert movie id to consecutive index and vice versa\n",
    "movieId_to_movieIDX = dict(zip(movieIds, range(0, m)))\n",
    "movieIDX_to_movieId = dict(zip(range(0, m), movieIds))\n",
    "# Dictionaries to convert user id to consecutive index and vice versa\n",
    "userId_to_userIDX = dict(zip(userIds, range(0, n)))\n",
    "userIDX_to_userId = dict(zip(range(0, n), userIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S56uwMG-t0lV"
   },
   "source": [
    "We therefore have an index (IDX) and an ID for each movie (item) and each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH27TM5cpdN2",
    "outputId": "dc899188-5671-4a97-8d85-739f17b37388"
   },
   "outputs": [],
   "source": [
    "# Users id to index\n",
    "for id in range(0, 10):\n",
    "    try:\n",
    "        print(id, userId_to_userIDX[id])\n",
    "    except:\n",
    "        print('There is nobody with id = ' + str(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the user indices, movie indices, and associated rating in the `ratings` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"cleaned\" dataframe with userIDX, movieIDX, and rating\n",
    "ratings = pd.concat([ratings_raw['userId'].map(userId_to_userIDX),\n",
    "                     ratings_raw['movieId'].map(movieId_to_movieIDX),\n",
    "                     ratings_raw['rating']], axis=1)\n",
    "ratings.columns = ['user', 'item', 'rating']\n",
    "\n",
    "display(ratings.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-0vV_MvmMZ_"
   },
   "source": [
    "### Create the Ratings Matrix\n",
    "\n",
    "We convert the `ratings` dataframe into a **Ratings Matrix**. Because it is very sparse, we use the `scipy.sparse` module to efficiently store and access it ([Documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html)).\n",
    "\n",
    "Specifically, we create **two** versions of the same ratings matrix:\n",
    "- `R` is our basic matrix and is optimized for dot products, which will be useful when computing user-user similarities; `R` is stored in the Compressed Sparse Row format (`csr_matrix`).\n",
    "- `R_dok` is a different view of the ratings matrix, which allows to quickly test whether a user-item rating exists; `R_dok` is stored in the Dictionary Of Keys format (`dok_matrix`) so you can access the data like a dictionary (which is fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0EyJSN2mMZ_",
    "outputId": "30952c20-be3b-4a78-e15e-2629d2bb9196"
   },
   "outputs": [],
   "source": [
    "# Create matrices\n",
    "R = sp.csr_matrix((ratings.rating, (ratings.user, ratings.item))) # input is (data, (rows, columns))\n",
    "R_dok = R.todok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0nWfDQ7mMZ_",
    "outputId": "c0bae6a9-bc38-4e9e-a7ee-a0ece7604760"
   },
   "outputs": [],
   "source": [
    "# Check dimensions\n",
    "m = R.shape[1]\n",
    "n = R.shape[0]\n",
    "numRatings = R.count_nonzero()\n",
    "print(f'There are {n} users, {m} movies, and {numRatings} ratings.')\n",
    "\n",
    "# A simple test: user 0 item 124 should have a rating of 5\n",
    "print(\"R[0, 124] value is \", R[0, 124])\n",
    "print(\"R_dok[(0, 124)] value is \", R_dok[(0, 124)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Le4YNQmMZ_"
   },
   "source": [
    "The fun starts here! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ov2ihAWmMZ_"
   },
   "source": [
    "### User Average Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2MgtshPmMZ_"
   },
   "source": [
    "Let's first compute the average rating of each user. This will be useful for mean-centering, i.e., when computing similarities, as well as for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-j329hh3mMZ_",
    "outputId": "49e72da6-cf32-4767-ba93-91b356168e03"
   },
   "outputs": [],
   "source": [
    "# For each user, sum ratings \n",
    "user_sums = R.sum(axis=1).A1       # axis = 1 sums over columns, A1 to convert matrix to 1-D array\n",
    "# Count number of movies rated for each user\n",
    "user_cnts = np.diff(R.indptr)\n",
    "# Compute average\n",
    "user_avgs = user_sums / user_cnts\n",
    "print(\"User averages:\", user_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrcGaqpVmMZ_"
   },
   "source": [
    "This [stack overflow thread](https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr) explains the method `indptr` for a sparse matrix in scipy clearly. Recommended to read for those who are inerested or got confused with the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcfoifKSmMZ_"
   },
   "source": [
    "### Compute User-User Similarity \n",
    "\n",
    "We will compute the **mean-centered cosine similarity** between two users $u$ and $v$:\n",
    "$$w_{uv}=\\frac{\\sum_{i \\in I_u \\cap I_v}(r_{ui}-\\bar{r_u})(r_{vi}-\\bar{r_v})}{\\sqrt{\\sum_{i \\in I_u}(r_{ui}-\\bar{r_u})^2}\\sqrt{\\sum_{i \\in I_v}(r_{vi}-\\bar{r_v})^2}}= \\frac{\\langle \\overrightarrow{u} , \\overrightarrow{v} \\rangle}{||\\overrightarrow{u}|| \\cdot ||\\overrightarrow{v}||} $$\n",
    "\n",
    "where $r_{ui}$ is the rating for user $u$ and movie $i$, $\\bar{r_u}$ is the average rating of user $u$, and $I_u$ is the set of movies rated by user $u$, and $\\overrightarrow{u}$ is the mean-centered ratings vector of user $u$. \n",
    "\n",
    "**Note**: from now on, we work with `R` and `R_dok`. This means that we work with the index IDX, not the identifiers anymore. \n",
    "\n",
    "*Useful numpy tricks*:\n",
    "- To subtract a scalar value `a` from all nonzero entries of a sparse vector `x`, do: `x.data = x.data - a`\n",
    "- The dot product of a sparse vector `x` to sparse vector `y` is: `x.dot(y.T)`\n",
    "- The norm of a sparse vector `x` is: `norm(x)`\n",
    "- If a sparse vector `x` has only a single item, you can access it by: `x.A.item()`. Note that `x.A` returns the dense representation of sparse vector `x`.\n",
    "\n",
    "Let's illustrate these tricks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0BSmak3mMZ_",
    "outputId": "bedcf152-f1e4-4897-8186-e3d8599e832e"
   },
   "outputs": [],
   "source": [
    "u = R[0,:].copy()  # User 0 ratings\n",
    "v = R[1,:].copy()  # User 1 ratings\n",
    "print('Data type:', type(u.data))\n",
    "print('Dot product:', u.dot(v.T))\n",
    "print('Dot product as matrix:', u.dot(v.T).A)\n",
    "print('Dot product value:', u.dot(v.T).A.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that it is clear, we define a function to compute the pairwise mean-centered cosine similarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u_tGHy9mMaA"
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_user_similarity(u_id, v_id):\n",
    "    \"\"\"Computes the cosine similarity between two user IDX.\n",
    "            sim(u,v)=dot(u,v)/norm(u)*norm(v)\n",
    "    \"\"\"\n",
    "    # Extract users ratings\n",
    "    u = R[u_id,:].copy()    # User u ratings\n",
    "    v = R[v_id,:].copy()    # User v ratings\n",
    "    \n",
    "    # Compute numerator, i.e., dot product of the mean centered arrays\n",
    "    u.data = u.data - user_avgs[u_id]   # Mean-centered ratings vector of user u\n",
    "    v.data = v.data - user_avgs[v_id]   # Mean-centered ratings vector of user v    \n",
    "    numerator =  (u.dot(v.T)).A.item()  # Value of dot product\n",
    "    \n",
    "    # Compute demoninator, i.e., product of the norms\n",
    "    denominator = norm(u) * norm(v)\n",
    "    \n",
    "    # Compute similarity\n",
    "    if denominator == 0:\n",
    "        similarity = 0.;\n",
    "    else:\n",
    "        similarity = numerator/denominator\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp4wOE0QmMaA"
   },
   "source": [
    "The similarity will be between -1 and +1, from very dissimilar to very similar. Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqMUMqVFmMaA",
    "outputId": "8ea8baad-6c2e-4788-f4e4-5358d3cd92ff"
   },
   "outputs": [],
   "source": [
    "sim = compute_pairwise_user_similarity(3, 9) # Similarity of user 3 and user 9\n",
    "print(round(sim, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmZt2vQEmMaA"
   },
   "source": [
    "### User to all Users Similarities\n",
    "\n",
    "The following functions compute the mean-centered cosine similarities of a given user to all other users. We'll try two techniques:\n",
    "1. We first use the `compute_pairwise_user_similarity` function defined above, and loop through all other users;\n",
    "2. We try to avoid the for loop and not invoke `compute_pairwise_user_similarity`. The idea is to obtain a copy, `R_copy`, of matrix `R` that has its rows mean-centered and normalized. This way the given user can be represented by a mean-centered and normalized vector `u`. Then, to obtain the similarity of the user to all others, one needs to take the dot product `R_copy.dot(u.T)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myKa5Gn8mMaA"
   },
   "outputs": [],
   "source": [
    "# Approach 1: Loop and pairwise_user_similarity\n",
    "def compute_user_similarities(u_id):\n",
    "    uU = np.empty(n)     # Array of n users\n",
    "    # Loop through users\n",
    "    for v_id in range(n):\n",
    "        sim = compute_pairwise_user_similarity(u_id, v_id)\n",
    "        uU[v_id] = sim   \n",
    "    return uU\n",
    "\n",
    "# Approach 2: Mean-center normalized R & dot product\n",
    "def compute_user_similarities_fast(u_id):\n",
    "    uU = np.empty(n)\n",
    "    R_copy = R.copy()\n",
    "    # Repeat each user_avg, user_cnt times (number of movies rated for each user)\n",
    "    user_avgs_repeated = np.repeat(user_avgs, user_cnts)\n",
    "    # Mean-center R_copy\n",
    "    R_copy.data -= user_avgs_repeated    \n",
    "    # Normalize rows to unit norm\n",
    "    R_copy = pp.normalize(R_copy, axis=1) # normalize each row: elements divided by the row norm\n",
    "    # Extract ratings of user u\n",
    "    u = R_copy[u_id, :]\n",
    "    # Compute simarity (dot product)\n",
    "    uU = R_copy.dot(u.T).A.flatten() # dot product, convert to dense matrix, then flatten\n",
    "    return uU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4vu1283mMaA"
   },
   "source": [
    "Let's time our two functions to check the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGWJcCprmMaA",
    "outputId": "4c59ddc3-dd9c-4b82-fa42-842444744bb5"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "uU = compute_user_similarities(25)\n",
    "end = time.time()\n",
    "print(\"Time for the first function with for loop=\", (end-start))\n",
    "    \n",
    "start = time.time()\n",
    "uU = compute_user_similarities_fast(25)\n",
    "end = time.time()\n",
    "print(\"Time for second function using R_copy=\", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our second function is much faster!\n",
    "\n",
    "Let's explore a bit more the similarity between user index 25 and all other users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0fEuvUqmMaA",
    "outputId": "391c8325-2d4e-4e40-bb9c-f9e641ff9c7e"
   },
   "outputs": [],
   "source": [
    "print('Dimension of uU:', uU.shape)\n",
    "print('Similarity of user IDX 25 to all the other users:', uU)\n",
    "print('Similarity between users IDX 25 and IDX 74:', round(uU[74], 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most similar user to user index 25. We sort our similarity array and extract the penultimate user index (since the last one is user 25 with a similarity of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u25_most_similar = np.argsort(uU)[-2]\n",
    "print(f'The user most similar to user index 25 is: {u25_most_similar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wibGe5amMaA"
   },
   "source": [
    "Now let's compare the top 10 rated movies by user index 25 and user index 511, who is the most similar user to user index 25. We use our previously-defined function to show the movie posters of the top rated movies of a user. Note that we have to convert user index IDX to user identifiers, using our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "c9mYhs0ZmMaA",
    "outputId": "92137444-6215-49db-93a1-58d46c07719b"
   },
   "outputs": [],
   "source": [
    "show_movies_for_user(userIDX_to_userId[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_movies_for_user(userIDX_to_userId[u25_most_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few similar movies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5CkBPfGmMaA"
   },
   "source": [
    "### Create User Neighborhood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kSjCeZ5mMaA"
   },
   "source": [
    "We're getting close to finalizing our recommender! The next step is to create the user neighborhood of a given user. \n",
    "\n",
    "We define another function that takes as input, the target user `u_idx` and the target item `i_idx` (i.e., movie), and uses as additional parameters the size `k` of the neighborhood and a flag `with_abs_sim`:\n",
    "- If `with_abs_sim` is `True`, the neighborhood should contain up to `k` users with the highest absolute similarity to the target user `u_idx`.\n",
    "- If `with_abs_sim` is `False`, the neighborhood should contain up to `k` users with the highest similarity to the target user `u_idx`.\n",
    "\n",
    "The output of the function is `nh`, a `Dictionary` containing key-value entries of the form `v_idx : sim(u_idx, v_idx)`, where `v_idx` is another user and `sim(u_idx, v_idx)` is the similarity between `u_idx` and `v_idx`.\n",
    "\n",
    "**Note:** The neighborhood of the target user should not contain itself, i.e., `u_idx`, and only include users that have rated the target item `i_idx`.\n",
    "\n",
    "*Useful tricks*:\n",
    "- `np.absolute(x)` returns an array containing the absolute values of each element in array `x`.\n",
    "- `np.argsort(x)` returns an array with the indices that sort array `x` in *increasing* order.\n",
    "- `x[::-1]` returns the reversed array of `x`. So, `np.argsort(x)[::-1]` contains the indices that sort x in *decreasing* order.\n",
    "- To check if user `u_idx` has rated item `i_idx`, the `R_dok` view of the ratings matrix is helpful: `(u_idx, i_idx) in R_dok`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBNqDjaDmMaA"
   },
   "outputs": [],
   "source": [
    "def create_user_neighborhood(u_idx, i_idx, k=5, with_abs_sim=False):\n",
    "    \"\"\" Neighborhood for user \"u_idx\" that have also watched item \"i_idx\"\n",
    "    \"\"\"\n",
    "    nh = {}  # the neighborhood dict with (user idx: similarity) entries\n",
    "    \n",
    "    # Compute similarities\n",
    "    uU = compute_user_similarities_fast(u_idx)\n",
    "    uU_copy = uU.copy() ## so that we can modify it, but also keep the original\n",
    "    \n",
    "    # Absolute similarity\n",
    "    if with_abs_sim:\n",
    "        uU_copy = np.absolute(uU_copy)  # we only care about the absolute value of the similarity\n",
    "    \n",
    "    # Derive the user_ids sorted by decreasing similarity (or absolute similarity) to u_id\n",
    "    user_ids =  np.argsort(uU_copy)[::-1]\n",
    "    \n",
    "    # Create neighborhood\n",
    "    count = 0\n",
    "    for v_idx in user_ids:\n",
    "        if v_idx == u_idx: # ignore self\n",
    "            continue     # go to the next iterate of the loop \n",
    "        if (v_idx, i_idx) not in R_dok:   # ignore users that have not rated i_idx\n",
    "            continue\n",
    "        nh[v_idx] = uU[v_idx]\n",
    "        count += 1\n",
    "        if count == k:  # at most k neighbors\n",
    "            break\n",
    "    \n",
    "    return nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our function for user index 25 and their top rated movie, namely \"Pulp Fiction\", which has a movie identifier of 296. Note that we need to convert the movie identifier to the movie index using our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otmCwemxmMaA",
    "outputId": "ea5b424d-b1a4-4e2e-ecce-934b4f7d54c9"
   },
   "outputs": [],
   "source": [
    "# Compute neighborhood for user (idx) 25 and movie (id) 296 (Pulp Fiction)\n",
    "nh = create_user_neighborhood(u_idx=25, i_idx=movieId_to_movieIDX[296])\n",
    "print(f'Neighborhood of user 25 for {get_name_for_movie_id(movie_id=296)}: {nh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTl-1zqlmMaA"
   },
   "source": [
    "### Predict a Rating \n",
    "\n",
    "Finally! We can now try to predict a rating of a given user who has not rated an item so far.\n",
    "\n",
    "The following function predicts the rating user `u_idx` would give to item `i_idx`. It uses the flag `with_deviations` to make the prediction:\n",
    "- If `with_deviations` is `True`, the prediction is made over *rating deviations*:\n",
    "$$ s(u,i) = \\overline{r_u} + \\frac{\\sum_{v \\in N(u;i)}w_{uv} (r_{vi}-\\overline{r_v})}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
    "- If `with_deviations` is `False`, the prediction is made directly over ratings:\n",
    "$$ s(u,i) = \\frac{\\sum_{v \\in N(u;i)}w_{uv} r_{vi}}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
    "\n",
    "The output of the function is the predicted rating `prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Sy7a_5UmMaA"
   },
   "outputs": [],
   "source": [
    "def predict_rating(u_idx, i_idx, k, with_deviations=True, with_abs_sim=False):\n",
    "    '''\n",
    "    Predict the rating of user u_idx for item i_idx \n",
    "    '''\n",
    "    \n",
    "    print(\"Movie name:\", get_name_for_movie_id(movie_id=movieIDX_to_movieId[i_idx]))\n",
    "    \n",
    "    if (u_idx, i_idx) in R_dok:\n",
    "        print(f'User idx {u_idx} has rated movie idx {i_idx} with {R[u_idx, i_idx]}.')\n",
    "    else:\n",
    "        print(f'User idx {u_idx} has not rated movie idx {i_idx}.')\n",
    "    print(f'k: {k}, with_deviations: {with_deviations}, with_abs_sim: {with_abs_sim}')\n",
    "    \n",
    "    # Neighborhood\n",
    "    nh = create_user_neighborhood(u_idx, i_idx, k=k, with_abs_sim=with_abs_sim)\n",
    "\n",
    "    # Compute neighborhood weighted average (fraction in prediction formula)\n",
    "    sum_scores = 0.      # numerator\n",
    "    sum_weights = 0.     # denominator\n",
    "    for neighbor_idx, similarity in nh.items():\n",
    "        # Find the neighbor rating from R matrix.\n",
    "        neighbor_rating = R[neighbor_idx, i_idx]\n",
    "        if with_deviations:\n",
    "            # In this case similarity should be multiplied by (neighbor_rating - neighbor_avg)\n",
    "            sum_scores += similarity * (neighbor_rating - user_avgs[neighbor_idx])\n",
    "        else:\n",
    "            # In this case we do not have the average\n",
    "            sum_scores += similarity * neighbor_rating\n",
    "        sum_weights += abs(similarity)\n",
    "    neighborhood_weighted_avg = sum_scores/sum_weights\n",
    "    \n",
    "    # Compute and print prediction\n",
    "    if with_deviations:\n",
    "        prediction = user_avgs[u_idx] + neighborhood_weighted_avg\n",
    "        print(f'Prediction: {prediction} (user average: {user_avgs[u_idx]}, offset: {neighborhood_weighted_avg})')\n",
    "    else:\n",
    "        prediction = neighborhood_weighted_avg\n",
    "        print(f'Prediction: {prediction} (user average: {user_avgs[u_idx]})')\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMa5Di5ImMaA"
   },
   "source": [
    "Let's try to predict the ratings that user index 25 would give to some movies. First, we'll see how our prediction compares to the rating user index 25 gave to Pulp Fiction (4.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4s_6CFfmMaA",
    "outputId": "1319e421-231a-45c4-9781-995b47477849"
   },
   "outputs": [],
   "source": [
    "predict_rating(25, movieId_to_movieIDX[296], k=50, with_deviations=True, with_abs_sim=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the rating for a movie not seen by user index 25, such as the Lion King:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating(25, movieId_to_movieIDX[364], k=50, with_deviations=True, with_abs_sim=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLWjvjQc1MjX"
   },
   "source": [
    "### Recommendation\n",
    "\n",
    "We will recommend 5 movies to a user. Of course, the user must not have already watched the movies.\n",
    "Let's select one user index, e.g., 91. Feel free to explore other users (you can pick any integer between 0 and 609)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "Puvko-HOw7yT",
    "outputId": "2d4af2d5-d97c-4f15-f345-172e9ea701f0"
   },
   "outputs": [],
   "source": [
    "# Choose a user (IDX) - between 0 and 609.\n",
    "user_IDX = 91\n",
    "\n",
    "# Print user ID using dictionary\n",
    "u_id = userIDX_to_userId[91]\n",
    "\n",
    "# Display movies that user has already watched\n",
    "show_movies_for_user(u_id)\n",
    "\n",
    "# Show genre histogram of the user\n",
    "show_genres_histogram_for_user(u_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our predictor function to predict the ratings user index 91 would give to movies they have not watched. We create a dictionary (i_id, rating) with the predicted ratings for the movies, and loop through our first 50 movies index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHUdN39-2NMU",
    "outputId": "020d4898-fdfb-4b4c-8f6c-1a8679707947",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_predicted = {}\n",
    "for i_id in range(50):\n",
    "    if (u_id, i_id) not in R_dok: # user should not have already watched the movie\n",
    "        #Predict rating of user for movie i_id\n",
    "        ratings_predicted[i_id] = predict_rating(u_id, i_id, k=50, with_deviations=True, with_abs_sim=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the top 5 predicted rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ge_nCTGFAN00",
    "outputId": "7e5d9c1f-78c3-4343-850c-24c631350cce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort dictionary and print top 5 movies\n",
    "top_ratings = sorted(ratings_predicted.values(), reverse=True)[:5] # first 5 movies\n",
    "for idx, rating in ratings_predicted.items():  \n",
    "    if rating in top_ratings:\n",
    "        print(get_name_for_movie_id(movieIDX_to_movieId[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynERFI7LQDlO"
   },
   "source": [
    "## Using the Surprise library\n",
    "\n",
    "[Surprise](http://surpriselib.com/) is a Python [scikit](https://projects.scipy.org/scikits.html) for building and analyzing recommender systems that deal with explicit rating data. You can install the package using the following code line in your terminal:\n",
    "\n",
    "```python\n",
    "pip install scikit-surprise\n",
    "```\n",
    "\n",
    "Alternatively, if pip does not work, you can use conda:\n",
    "\n",
    "```python\n",
    "conda install -c conda-forge scikit-surprise\n",
    "```\n",
    "\n",
    "We will use the 100k dataset from MovieLens, which is already available in the Surprise library. Information about the dataset is available [here](https://grouplens.org/datasets/movielens/).\n",
    "\n",
    "If you get the error `SSL: CERTIFICATE_VERIFY_FAILED` when importing the dataset, try the following code:\n",
    "```python\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get the error SSL: CERTIFICATE_VERIFY_FAILED when importing the dataset, \n",
    "# uncomment the following:\n",
    "\n",
    "'''\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context    \n",
    "'''\n",
    "\n",
    "# Load data\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Number of movies\n",
    "print('The dataset contains {} movies.'.format(len(trainset.all_items())))\n",
    "\n",
    "# Number of movies\n",
    "print('The dataset contains {} users.'.format(len(trainset.all_users())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a KNN model. The `KNNWithMeans` function is using as parameter:\n",
    "- `k`: (maximum) number of neighbors\n",
    "- `min_k`: minimum number of neighbors\n",
    "- `sim_options`: dictionary of options for the similarity measure. We are going to use:\n",
    "    - `'name': 'pearson'`: Pearson similarity which can be seen as mean-centered cosine similarity,\n",
    "    - `'user_based': True`: User-based collaborative filtering\n",
    "- `verbose`: whether to print trace messages of bias estimation, similarity, etc. \n",
    "\n",
    "Check the [Documentation](https://surprise.readthedocs.io/en/stable/knn_inspired.html) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aKb5IkH1bIQ"
   },
   "outputs": [],
   "source": [
    "# Define options\n",
    "sim_options = {\n",
    "    'name': 'pearson', \n",
    "    'user_based': True \n",
    "}\n",
    "\n",
    "# Create instance of class\n",
    "knn_means = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model and compute predictions on the test set (Note that the training might take a bit of time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v83f8VHP1bGj"
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "knn_means.fit(trainset)\n",
    "\n",
    "# Predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset() \n",
    "predictions = knn_means.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the predictions look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XEJj5f82-6g",
    "outputId": "f1109936-0fb6-4411-f1e9-19b022040f85"
   },
   "outputs": [],
   "source": [
    "# Examples of predictions\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the output we get:\n",
    "- `uid` is the user id\n",
    "- `iid` is the item id\n",
    "- `r_ui` is the true rating\n",
    "- `est` is the estimated rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_rhhcdM33s1"
   },
   "source": [
    "That's it!. In less than 10 lines, we managed to do the same as in the first part... Packages are great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGCx7IflRsgR"
   },
   "source": [
    "### Visualizing the recommendations\n",
    "\n",
    "We have built the predictions. Now we can visualize them. We first write these helpers functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_item_names():\n",
    "    '''Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    '''\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list) # This is used to group a sequence of key-value pairs into a dictionary of lists\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for each user what are the top recommended movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TtlypKiCRkyx",
    "outputId": "a53cfe1b-49d7-4dfd-e13b-4c0694f661c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get top 10 movies for all users\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Read the mappings row id <-> movie name\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# Print the recommended items for user id 135\n",
    "uid = '135'\n",
    "user_ratings = top_n[uid]\n",
    "recommended_items = [iid for (iid, _) in user_ratings]\n",
    "print(f'Recommended items for user id {uid}: ',', '.join(recommended_items))\n",
    "\n",
    "# Convert ids into names\n",
    "item_names = [rid_to_name[rid]\n",
    "              for rid in recommended_items]\n",
    "print(f'\\nRecommended movies for user id {uid}: ', ', '.join(item_names))\n",
    "\n",
    "print('\\nMovies list:')\n",
    "# Show name, url and covers\n",
    "for name in item_names:\n",
    "    print('\\nName: ', name)\n",
    "    clean_name = re.sub(r'\\([^)]*\\)', '', name) # Remove year of the movie (in between paranthesis)\n",
    "    try:\n",
    "        url = TMDB_class.TMDB().get_poster_path_by_name(clean_name)\n",
    "    except:\n",
    "        url = None\n",
    "    print('url: ', url)\n",
    "    if url:\n",
    "        display(Image(url=url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPus0WuNQTu-"
   },
   "source": [
    "### Precision and Recall at rank k\n",
    "\n",
    "We can evaluate the performance of our recommender using the precision and recall at rank k. Let's first create a function that returns the precision and recall at rank k for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YygiDmbSGeC"
   },
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the same model as before, this time splitting our dataset into training and test set. Then, we compute the precision and recall for various ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "t34ttk2LSRiP",
    "outputId": "520e4cfe-14ad-4ec3-f466-446f4c1f0887"
   },
   "outputs": [],
   "source": [
    "# Create training and test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Instance of KNNWithMeans\n",
    "algo = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
    "\n",
    "# Fit model on training set\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict test set values\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Compute precision and recall for k between 0 and 9\n",
    "precision = []\n",
    "recall = []\n",
    "for k in range(10):\n",
    "    # rating > 3.5 = relevant, rating < 3.5 = irrelevant\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5)\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
    "    recall.append( sum(rec for rec in recalls.values()) / len(recalls) )\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(10), recall, 'ro-', label=\"Recall\")\n",
    "plt.plot(range(10), precision, 'go-', label=\"Precision\")\n",
    "plt.legend()\n",
    "plt.title(\"Precision and recall for user-based KNN\")\n",
    "plt.show();\n",
    "\n",
    "# Precision and recall at rank 20:\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=20, threshold=3.5)\n",
    "print(\"Precision @ 20 for user-based knn\", sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"Recall @ 20 for user-based knn\", sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl6U1X9tSWS2"
   },
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "We will now observe the area under precision recall curve for two methods: SVD ([Documentation](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)) and KNN. We first create a function that takes as input an instance of algorithm and returns the precision and recall as calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_algo(algo):\n",
    "    '''Return precision and recall at k metrics for an algorithm.'''    \n",
    "    \n",
    "    # Fit algo on training set\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    # Predict on test set\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for k in range(20):\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5)\n",
    "        precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
    "        recall.append( sum(rec for rec in recalls.values()) / len(recalls) ) \n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our precision-recall curves for our two algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "pLLQAQ3JST84",
    "outputId": "cb8e36fd-f803-4952-cb76-8ea72de86a89"
   },
   "outputs": [],
   "source": [
    "# KNN precision and recall\n",
    "algo_KNN =  KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
    "precision_KNN, recall_KNN = precision_recall_algo(algo_KNN)\n",
    "\n",
    "# SVD precision and recall\n",
    "algo_SVD = SVD()\n",
    "precision_SVD, recall_SVD = precision_recall_algo(algo_SVD)\n",
    "\n",
    "# Plot\n",
    "plt.step(recall, precision, color='b', where='post', label ='KNN')\n",
    "plt.step(recall_SVD, precision_SVD, color='g', where='post', label ='SVD')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "The Surprise library also provides built-in cross-validation to split the data to multiple folds ([Documentation](https://surprise.readthedocs.io/en/stable/model_selection.html#cross-validation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define options\n",
    "sim_options = {\n",
    "    'name': 'pearson',\n",
    "    'user_based': True \n",
    "}\n",
    "\n",
    "# Create instance of KNNWithMeans\n",
    "knn_means = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(knn_means, data, measures=['RMSE'], cv=5, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7MDnHBBSdhi"
   },
   "source": [
    "### Tuning hyper-parameters\n",
    "\n",
    "We can also use `GridSearchCV` ([Documentation](https://surprise.readthedocs.io/en/stable/model_selection.html#surprise.model_selection.search.GridSearchCV)) to tune the hyper-parameters of our recommender system, e.g., tuning the number of neighbours in KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hst8xKOvSokf",
    "outputId": "198bbc4d-5f21-42b1-8a31-d196585b4008"
   },
   "outputs": [],
   "source": [
    "param_grid={'k': [20, 30, 40, 50], \n",
    "            'sim_options': {'name': ['pearson'], 'user_based': [True]}}\n",
    "\n",
    "KNN_grid_search = GridSearchCV(KNNWithMeans, param_grid=param_grid, \n",
    "                               measures=['RMSE'], cv=5,\n",
    "                               refit=True, joblib_verbose=2, n_jobs=-1)\n",
    "\n",
    "KNN_grid_search.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbzLXSHxSp_X",
    "outputId": "8a393f62-c70b-4992-9806-e777deb3342e"
   },
   "outputs": [],
   "source": [
    "print('Best parameter:', KNN_grid_search.best_params)\n",
    "print('Best RMSE: ', KNN_grid_search.best_score)\n",
    "# We can even see the whole cv results\n",
    "KNN_grid_search.cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7WFcS8eStYP"
   },
   "source": [
    "Finally, let's extract the best model and predict the rating given by user id 6 to movie id 908:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPD_sr1mSrW2"
   },
   "outputs": [],
   "source": [
    "best_model_knn = KNN_grid_search.best_estimator['rmse']\n",
    "best_model_knn.predict('6', '908')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Recommender_systems.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
