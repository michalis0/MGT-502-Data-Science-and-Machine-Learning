{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "# ML import\n",
    "from sklearn import datasets        # datasets\n",
    "from sklearn.cluster import KMeans  # K-Means algorithm\n",
    "from sklearn.cluster import AgglomerativeClustering  # Hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # dendogram visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e553-36af-42c8-9522-dd090686c913",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:4800/format:webp/0*ZxLMBwq9rmW9ZFuZ.jpg' width=\"800\">\n",
    "\n",
    "Source: [The difference between supervised and unsupervised learning](https://twitter.com/athena_schools/status/1063013435779223553), illustrated by [@Ciaraioch](https://twitter.com/Ciaraioch) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on clustering, focusing on two methods: K-Means and Hierarchical clustering. After presenting the main concepts, you will be introduced to the techniques to implement the algorithms in Python. Finally, it will be your turn to practice, using an application on customers of shopping mall.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [Background](#Background)\n",
    "    - [Objective](#Objective)\n",
    "    - [Algorithm overview](#Algorithm-overview)\n",
    "- [Implementation](#Implementation)\n",
    "    - [Discover dataset](#Discover-dataset)\n",
    "    - [K-Means](#K-Means)\n",
    "        - [Implementing K-Means](#Implementing-K-Means)\n",
    "        - [Graphical representation](#Graphical-representation)\n",
    "        - [Elbow method](#Elbow-method)\n",
    "    - [Hierarchical clustering](#Hierarchical-clustering)\n",
    "        - [Implementing hierarchical (agglomerative) clustering](#Implementing-hierarchical-(agglomerative)-clustering)\n",
    "        - [Dendrogram visualization](#Dendrogram-visualization)\n",
    "    - [Runtime complexity](#Runtime-complexity)\n",
    "- [Your turn](#Your-turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5570d-cf11-4ef6-90cd-f5bdef3a8d23",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Objective\n",
    "\n",
    "Clustering aims at creating groups of data points with the goal to:\n",
    "- organize data into classes with high intra-class similarity and low inter-class similarity\n",
    "- find the class labels and the number of classes directly from the data (vs classification for which classes are defined)\n",
    "- find natural groupings among objects\n",
    "\n",
    "Clustering algorithms are thus **unsupervised learning** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8862104-1f60-4b53-91e3-959afa7494e1",
   "metadata": {},
   "source": [
    "### Algorithm overview\n",
    "\n",
    "Here is a table describing different techniques available with the sklearn module `sklean.cluster`. The [documentation](https://scikit-learn.org/stable/modules/clustering.html) contains detailed description of each technique, you can explore it to deepen your understanding of each algorithm! You can also refer to the [Glossary](https://scikit-learn.org/stable/glossary.html) for definitions of technical terms.\n",
    "\n",
    "| Method name | Parameters | Usecase | Geometry (metric used) |\n",
    "| :- | :- | :- | :- |\n",
    "| [K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means) | Number of clusters | General-purpose, even cluster size, flat geometry, not too many clusters, inductive | Distances between points|\n",
    "| [Affinity propagation](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) | Damping, sample preference | Many clusters, uneven cluster size, non-flat geometry, inductive | Graph distance (e.g. nearest-neighbor graph)|\n",
    "| [Mean-shift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift) | Bandwidth | Many clusters, uneven cluster size, non-flat geometry, inductive | Distances between points|\n",
    "| [Spectral clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering) | Number of clusters | Few clusters, even cluster size, non-flat geometry, transductive | Graph distance (e.g. nearest-neighbor graph)|\n",
    "| [Ward hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) | Number of clusters or distance threshold | Many clusters, possibly connectivity constraints, transductive | Distances between points|\n",
    "| [Agglomerative clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) | Number of clusters or distance threshold, linkage type, distance| Many clusters, possibly connectivity constraints, non Euclidean distances, transductive | Any pairwise distance|\n",
    "| [DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan) | Neighborhood size | Non-flat geometry, uneven cluster sizes, outlier removal, transductive | Distances between nearest points|\n",
    "| [OPTICS](https://scikit-learn.org/stable/modules/clustering.html#optics) | Minimum cluster membership | Non-flat geometry, uneven cluster sizes, variable cluster density, outlier removal, transductive | Distances between points|\n",
    "| [Gaussian mixtures](https://scikit-learn.org/stable/modules/mixture.html#mixture) | Many | Flat geometry, good for density estimation, inductive | Mahalanobis distances to  centers|\n",
    "| [BIRCH](https://scikit-learn.org/stable/modules/clustering.html#birch) | Branching factor, threshold, optional global clusterer | Large dataset, outlier removal, data reduction, inductive | Euclidean distance between points|\n",
    "| [Bisecting K-Means](https://scikit-learn.org/stable/modules/clustering.html#bisect-k-means) | Number of clusters | General-purpose, even cluster size, flat geometry, not too many clusters, inductive | Distances between points|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c7da7-5579-498a-884c-54e9425e5db8",
   "metadata": {},
   "source": [
    "Each method performs differently depending on the input data. They also differ in their complexity. The figure below illustrates these differences - check the computation time bottom right!\n",
    "\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png' width=\"800\">\n",
    "<center/>\n",
    "\n",
    "Source: [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a19ea-246b-4af3-bd04-885218a6ccff",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4274f-e942-4422-be1b-22805b54d3aa",
   "metadata": {},
   "source": [
    "### Discover dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a67de-6ef7-4473-894c-0cdf5a6c9129",
   "metadata": {},
   "source": [
    "We're going to use the Iris dataset, which contains measurement for 3 different types of iris flowers: *setosa*, *versicolor*, and *virginica*:\n",
    "\n",
    "<img src='https://machinelearninghd.com/wp-content/uploads/2021/03/iris-dataset.png' width=\"600\">\n",
    "\n",
    "The data includes, for each Iris flower, measures of width and length of sepals and petals. The dataset was originally created by Sir R.A. Fisher. We can directly obtain the dataset from `sklearn`, via the `datasets` module, which contains several toy datasets ([Documentation](https://scikit-learn.org/stable/datasets/toy_dataset.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "```\n",
    "\n",
    "The Iris dataset is a classic in ML and is often used to discover, for instance, classification, clustering, and dimensionality reduction. Let's discover it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cf6d2-18b1-4feb-b7d7-69b2c5ec235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from sklearn\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41adab-7423-4816-b84c-ebcd628cb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The different types of irises are:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5e48e-287a-41e8-a47a-4d6968aaf3f0",
   "metadata": {},
   "source": [
    "The Iris dataset is saved as a set of numpy arrays. We're going to transform it into a pandas dataframe.\n",
    "\n",
    "**Note:** We could also used the numpy array format for k-means Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bceb7-8086-4f02-b996-4749e03df271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our dataframe only includes data about the flowers, and NOT the actual type of flowers\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3df7c-c633-4e38-be40-faca60beed28",
   "metadata": {},
   "source": [
    "We have 4 different metrics stored in X. For now we'll work with only the sepal features: \"sepal lenght (cm)\" and \"sepal width (cm)\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336b445-63f3-4c63-a4b3-b9becb848118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with sepal features\n",
    "X_sepal = X.loc[:, [\"sepal length (cm)\",\"sepal width (cm)\"]]\n",
    "X_sepal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e3609-6109-4ab4-b240-f145d45d3f3b",
   "metadata": {},
   "source": [
    "The species are encoded by labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14754d17-d467-4dde-b18a-389e4b51d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Species are encoded as:\", iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d678b0d-7160-49a1-8fd4-f4d7555b7ae5",
   "metadata": {},
   "source": [
    "We save these labels in a dataframe, indicating for each observation (flower) which kind it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344a194-8cb4-4395-822b-2eebbd31fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with flower type (labels)\n",
    "y=pd.DataFrame(iris.target, columns=[\"Flower_type\"])\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1667308-859b-4303-ab07-df1cf4051dc8",
   "metadata": {},
   "source": [
    "Let's check how many observations we have for each type of flowers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a7487-9b43-47e0-8b39-1d417179fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbc040-1546-43d7-9904-4efd061ef5d8",
   "metadata": {},
   "source": [
    "Let's print some summary statistics, for each type of flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c713d6-7de7-4d51-bf35-9d68b294df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "pd.concat([X_sepal, y], axis=1).groupby(['Flower_type']).describe().loc[:,(slice(None),['max','min','mean'])].transpose().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc5570-ecdc-4293-a7e2-8326f43cd0c4",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4d7d7-18fb-4684-9bab-82e9603ed837",
   "metadata": {},
   "source": [
    "#### Implementing K-Means\n",
    "\n",
    "We are using the `KMeans` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans))\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "As parameters, we need to specify `n_clusters`, describing the number of clusters to form as well as the number of centroids to generate. For illustration, we're going to train two K-Means models and fit it on the sepal features (X_sepal):\n",
    "-   K-means with 3 clusters\n",
    "-   K-means with 5 clusters\n",
    "\n",
    "The one with 5 clusters is only for illustration, because we already know that there are only 3 different types of iris in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cde5a-2158-41dc-bdb6-bcc538201e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instace of KMeans and specify the number of clusters=3, \n",
    "# Random state help make sure we all have exactly the same results\n",
    "kmeans3 = KMeans(n_clusters=3, random_state=0, n_init='auto') #3 clusters\n",
    "\n",
    "# Fit the model on the set of features we previously labelled as X_sepal (NOT including the labels on the type of flowers)\n",
    "kmeans3.fit(X_sepal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfdc8c-9a6d-4bbc-9900-dd372430918f",
   "metadata": {},
   "source": [
    "We can access the `labels_` of each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7be4b5-5c17-4ae2-be75-3ceb703641cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans3.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c0f93-3f3e-4d3b-9cff-0ba3d6c33681",
   "metadata": {},
   "source": [
    "We can use `cluster_centers_` to obtain the coordinates of the centers generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b43e5-0bf1-4154-9c23-7d5f07604d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans3.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2970b70-23ff-4f32-9c72-b58e1e7c135e",
   "metadata": {},
   "source": [
    "Let's proceed similarly, this time with 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb882ac5-c645-488f-a21a-6d0ba52c39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of KMeans\n",
    "kmeans5=KMeans(n_clusters=5, random_state=0, n_init='auto') #5 clusters\n",
    "\n",
    "# Fit the model on the X features\n",
    "kmeans5.fit(X_sepal)\n",
    "\n",
    "# Labels\n",
    "print(kmeans5.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025878b4-286c-4ff4-a39d-1cfd6fc2ed13",
   "metadata": {},
   "source": [
    "#### Graphical representation\n",
    "\n",
    "Let's visualize the clusters created, and compare them with the \"true\" labels (flower types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6103cdb-53a3-440a-b354-0b02a22bbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Scatter plot, raw data with original labels\n",
    "ax[0].scatter(X_sepal[\"sepal length (cm)\"], # x-axis\n",
    "              X_sepal[\"sepal width (cm)\"],  # y-axis\n",
    "              c=y['Flower_type'],           # points colored by the different flower types\n",
    "              cmap='tab10')                 # choice of colors\n",
    "ax[0].set_xlabel(\"Sepal length (cm)\")       # label x-axis\n",
    "ax[0].set_ylabel(\"Sepal width (cm)\")        # label y-axis\n",
    "ax[0].set_title(\"Raw data with the original labels\")  # title\n",
    "\n",
    "# Scatter plot of clusters, KMeans Model with 3 Clusters\n",
    "ax[1].scatter(X_sepal[\"sepal length (cm)\"], \n",
    "              X_sepal[\"sepal width (cm)\"], \n",
    "              c=kmeans3.labels_,              # points colored by the labels created by the model\n",
    "              cmap='tab10')\n",
    "ax[1].scatter(kmeans3.cluster_centers_[:, 0],  # x-coordinates of cluster centroids\n",
    "              kmeans3.cluster_centers_[:, 1],  # y-coordinates of cluster centroids\n",
    "              c=\"red\",                        # color of centroids\n",
    "              marker='x')                     # marker of centroids\n",
    "ax[1].set_xlabel(\"Sepal length (cm)\")\n",
    "ax[1].set_ylabel(\"Sepal width (cm)\")\n",
    "ax[1].set_title(\"KMeans Model with 3 Clusters\")\n",
    "\n",
    "# Scatter plot of clusters, KMeans Model with 5 Clusters\n",
    "ax[2].scatter(X_sepal[\"sepal length (cm)\"], X_sepal[\"sepal width (cm)\"], c=kmeans5.labels_, cmap='tab10')\n",
    "ax[2].scatter(kmeans5.cluster_centers_[:, 0], kmeans5.cluster_centers_[:, 1], c=\"red\", marker='x')\n",
    "ax[2].set_xlabel(\"Sepal length (cm)\")\n",
    "ax[2].set_ylabel(\"Sepal width (cm)\")\n",
    "ax[2].set_title(\"KMeans Model with 5 Clusters\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)   # Space between plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6deee-585f-4b6d-8983-092668c64f69",
   "metadata": {},
   "source": [
    "Our K-Means model with 3 clusters recognizes well the *setosa* (top left cluster), however it struggles to distinguish the *versicolor* and *virginica* since these flowers have interwonen sepal length and width. Note that the original labels and the one created with our model do not match (e.g., *setosa* is label 0 in the original dataset, and 1 in our K-Means model), hence the colors in our plot differ.\n",
    "\n",
    "How do we assess the performance of our models? In the general case we do not have the target variable (unsupervised learning), so we cannot rely on the metrics used in classification such as the accuracy. Instead, we rely on other metrics such as the **inertia**, which is the sum of squared distances of samples to their closest cluster center, potentially weighted by the sample weights if provided. This is the cost function that the algorithm minimizes. Let's check the inertia of our K-Means models with 3 and 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5d53f-3836-4188-9685-83f3d4812bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The inertia of the K-Means model with 3 clusters is: {:0.2f}\".format(kmeans3.inertia_))\n",
    "print(\"The inertia of the K-Means model with 5 clusters is: {:0.2f}\".format(kmeans5.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384adee-0afc-477c-a4a4-ac357b399d64",
   "metadata": {},
   "source": [
    "Which model should we choose? In other words, how many clusters should we pick? We'll explore this question using the Elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e547a1-0f1f-424f-af84-f77b2bfdefb4",
   "metadata": {},
   "source": [
    "#### Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa49d6-427a-4a41-a706-6c9671421e96",
   "metadata": {},
   "source": [
    "We now try to find the \"optimal\" number of clusters using the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)).  \n",
    "\n",
    "This method consists in plotting the explained variation (e.g., **inertia**) as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. The intuition is that increasing the number of clusters will always improve the fit (explain more of the variation), since there are more parameters (more clusters). However, this will at some point result in **over-fitting**, with only minimal gains in the fit, which the elbow reflects. \n",
    "\n",
    "Let's try! We create a loop to iteratively train K-Means algorithms for different values of k, saving the parameter `inertia_` at each iteration. This time we will use all the features, i.e., sepal and petal length and width. We then plot the inertia compared to the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65c9ba-f470-4371-a428-ba8f85fbd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "nbr_clusters = range(2,11)\n",
    "\n",
    "for i in nbr_clusters:\n",
    "    km = KMeans(n_clusters=i, random_state=0, n_init='auto').fit(X)  # Create and fit model\n",
    "    inertias.append(km.inertia_)     # Store inertia\n",
    "\n",
    "# Plot      \n",
    "plt.plot(nbr_clusters, inertias, '-o')\n",
    "plt.xticks(nbr_clusters)\n",
    "plt.title('Elbow method for inertia')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153a486-bbaa-404e-bdba-5eaaa6503dbc",
   "metadata": {},
   "source": [
    "The elbow method tells us to select the cluster when there is a significant change in inertia (i.e., cost). In this case, 4 seems like the optimal number of clusters. From k=5 we see that the reduction in the cost function is much lower than for example for k=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85cdf35-60d6-4033-8405-b5540ae2437b",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fa8df-eec6-4e53-985e-cb9a539c6222",
   "metadata": {},
   "source": [
    "#### Implementing hierarchical (agglomerative) clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04220ada-8804-436d-8dc5-7d8bc0257c40",
   "metadata": {},
   "source": [
    "We are using the `AgglomerativeClustering` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering))\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "```\n",
    "As parameters, we specify:\n",
    "- `n_clusters`, number of clusters to find (default =2)\n",
    "    - Instead of specifying the number of clusters, we could also provide the `distance_threshold`, which is the linkage distance threshold at or above which clusters will not be merged. If this case, `n_clusters` must be `None` and `compute_full_tree` must be `True`.\n",
    "- `metric` is the type of distance metric used, e.g., \"euclidean\" (default), \"l1\", \"l2\", \"manhattan\", \"cosine\", or \"precomputed\"\n",
    "    - Note: be careful to the sklearn version you are using. `metric` was added since version 1.2. For previous versions, you can use `affinity` (or better update your sklearn version).\n",
    "- `linkage` is the linkage criterion to use:\n",
    "    - 'single': minimum of the distances between all observations of the two sets\n",
    "    - 'complete': maximum distances between all observations of the two sets\n",
    "    - 'average': average of the distances of each observation of the two sets\n",
    "    - 'ward' (default): minimizes the variance of the clusters being merged\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2519aa8-11df-4d96-9029-d7944933acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "agglomerative3 = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='average')\n",
    "\n",
    "# Fit model\n",
    "agglomerative3.fit(X_sepal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16e62-123d-4f90-835d-9946ad7585c9",
   "metadata": {},
   "source": [
    "As before we can access the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456b085-96d3-4560-a6ee-f624a2e1f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agglomerative3.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8e770-6528-423b-9a41-b5ab968882fd",
   "metadata": {},
   "source": [
    "Let's plot our clusters to visually compare our results to the K-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655c8ab-c1a9-4cfb-a15a-63ab0a4b68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Scatter plot, raw data with original labels\n",
    "ax[0].scatter(X_sepal[\"sepal length (cm)\"], # x-axis\n",
    "              X_sepal[\"sepal width (cm)\"],  # y-axis\n",
    "              c=y['Flower_type'],           # points colored by the different flower types\n",
    "              cmap='tab10')                 # choice of colors\n",
    "ax[0].set_xlabel(\"Sepal length (cm)\")       # label x-axis\n",
    "ax[0].set_ylabel(\"Sepal width (cm)\")        # label y-axis\n",
    "ax[0].set_title(\"Raw data with the original labels\")  # title\n",
    "\n",
    "# Scatter plot of clusters, KMeans Model with 3 Clusters\n",
    "ax[1].scatter(X_sepal[\"sepal length (cm)\"], \n",
    "              X_sepal[\"sepal width (cm)\"], \n",
    "              c=kmeans3.labels_,              # points colored by the labels created by the model\n",
    "              cmap='tab10')\n",
    "ax[1].scatter(kmeans3.cluster_centers_[:, 0],  # x-coordinates of cluster centroids\n",
    "              kmeans3.cluster_centers_[:, 1],  # y-coordinates of cluster centroids\n",
    "              c=\"red\",                        # color of centroids\n",
    "              marker='x')                     # marker of centroids\n",
    "ax[1].set_xlabel(\"Sepal length (cm)\")\n",
    "ax[1].set_ylabel(\"Sepal width (cm)\")\n",
    "ax[1].set_title(\"KMeans Model with 3 Clusters\")\n",
    "\n",
    "# Scatter plot of clusters, agglomerative clustering with 3 Clusters\n",
    "ax[2].scatter(X_sepal[\"sepal length (cm)\"], X_sepal[\"sepal width (cm)\"], c=agglomerative3.labels_, cmap='tab10')\n",
    "ax[2].set_xlabel(\"Sepal length (cm)\")\n",
    "ax[2].set_ylabel(\"Sepal width (cm)\")\n",
    "ax[2].set_title(\"Agglomerative Model with 3 Clusters\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)   # Space between plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dfbe6-1455-4f03-b2fb-1dd96b8f332b",
   "metadata": {},
   "source": [
    "#### Dendrogram visualization\n",
    "\n",
    "In this section, we're going to present a way to create a **Dendrogram**, using the `scipy.cluster.hierarchy` library ([Documentation](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)). We're going to use: \n",
    "- `dendogram`: to plot the dendogram ([Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram))\n",
    "- `linkage`: to specify the type of linkage between the clusters ([Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage))\n",
    "\n",
    "In `linkage`, we can specify the `metric` (e.g., 'euclidean') and the `method` (e.g., 'single', 'complete', 'average', or 'ward' - see the documentation for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7eac57-543e-4a92-a757-50315385b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the linkage method we want and the chosen distance metric.\n",
    "method_Z = 'average' \n",
    "Z = linkage(X, method = method_Z, metric = 'euclidean')\n",
    "\n",
    "# Single linkage\n",
    "plt.figure(figsize=(16, 4))\n",
    "dendrogram(Z) # Plot the dendogram according the linkage\n",
    "plt.title('Dendrogram - '+method_Z, fontsize=14)\n",
    "plt.xlabel('Index of observations', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a048b4-03b9-4050-b2f6-76bacf7b2324",
   "metadata": {},
   "source": [
    "Feel free to try different methods and visualize the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae992fd-baaf-41e8-abb1-e1b978f1c244",
   "metadata": {},
   "source": [
    "### Runtime complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707bbdb-dbc6-446d-a35f-a30aee5e4a05",
   "metadata": {},
   "source": [
    "Let's compare the computation time needed between K-means and hierarchical clustering for different numbers of points. To do so, we are using the `time` library ([Documentation](https://docs.python.org/3/library/time.html)).\n",
    "\n",
    "We'll start by creating clusters of points. We are using `make_blobs` to generate our dataset ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)), and will first define a function with input the number of data points and output the generated samples (X) and associated labels (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41891f56-f8c0-48cb-8fe2-157a48dc3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function that generates 3 clusters\n",
    "def generate_three_clusters(num_points):\n",
    "    centers = [(-15, -15), (0, 0), (15, 15)]\n",
    "    cluster_std = [2, 3, 2]\n",
    "    X, y = datasets.make_blobs(n_samples=num_points, cluster_std=cluster_std, centers=centers, n_features=3, random_state=1)\n",
    "    return X, y\n",
    "\n",
    "# Example with 100 points\n",
    "X, y = generate_three_clusters(100)\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10)\n",
    "plt.scatter(X[y == 2, 0], X[y == 2, 1], color=\"green\", s=10)\n",
    "plt.title('Number of points: 100')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253bfcb-a4ff-4b7b-a2fc-ccde1baa4062",
   "metadata": {},
   "source": [
    "Next we generate 3 clusters using the above-defined function for n = 100, 1000, 2500, 5000, 7500, 10000, 25000 points, storing the result in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ce0bb-083a-492d-b430-6fb71b8350e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list\n",
    "X_list = []\n",
    "# Define numbers of points\n",
    "num_points = [100, 1000, 2500, 5000, 7500, 10000, 25000]\n",
    "\n",
    "for n in num_points:\n",
    "    X, y = generate_three_clusters(n)  # Generate three clusters\n",
    "    X_list.append(X)                   # Append X to X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ffa68-4eb4-4c0e-98e9-22bd6b19d8f1",
   "metadata": {},
   "source": [
    "The first item of our list contains 100 points, the second 1000 points, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753364d-8354-4c61-bf5b-df46438f8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_list[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dee343-2f4a-4f0a-ad3d-b38d36d2f9fa",
   "metadata": {},
   "source": [
    "Now we create K-Means and hierarchical clustering models (with 3 clusters), and train the algorithm on the dataset generated above. We store the execution time in two lists, one for K-Means, the other for hierarchical clustering:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b9973-62f7-4d4c-ab18-6dbd0753d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record time in list\n",
    "k_means_time = []\n",
    "hc_time = []\n",
    "\n",
    "for X in X_list:\n",
    "    # K-Means\n",
    "    model = KMeans(n_clusters=3, n_init='auto')    # Create instance of KMeans class (with 3 clusters)\n",
    "    start = time.time()                            # Start recording time\n",
    "    model.fit(X)                                   # Fit the model on X\n",
    "    end = time.time()                              # End recording time\n",
    "    k_means_time.append(end-start)                 # Store the execution time in k_means_time\n",
    "    # Hierarchical clustering\n",
    "    model = AgglomerativeClustering(n_clusters=3)  # Create instance of AgglomerativeClustering class (3 clusters)\n",
    "    start = time.time()\n",
    "    model.fit(X)\n",
    "    end = time.time()\n",
    "    hc_time.append(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31af6c-11f9-4af5-ad74-6d85aa1e1492",
   "metadata": {},
   "source": [
    "Let's plot the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e312c-3ef9-4647-a2a3-e0bc65398fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(num_points, k_means_time, label='K-Means')\n",
    "plt.scatter(num_points, hc_time, label='Hierarchical clustering')\n",
    "plt.ylabel('Execution time')\n",
    "plt.xlabel('Number of observations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15490b16-a272-411e-8554-454a9678cd2e",
   "metadata": {},
   "source": [
    "We can see that for a small number of observations, K-Means takes a bit longer than hierarchical clustering. However, when the number of observations increase, hierarchical clustering takes much longer than K-Means. Indeed, the hierarchical clustering algorithm needs to compute the distance between each observation, and then iteratively between each cluster created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bfaa9-e4eb-4f1e-8752-2edb0f4b5cef",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Now it's your turn to practice. We will use a dataset on the customers of a shopping mall, exploring clustering the customers based on their annual income and spending score to see if there are distinguishable clusters which the mall can target.\n",
    "\n",
    "The dataset was obtained from Dr. Tirthajyoti Sarkar GitHub repository [Machine-Learning-with-Python](https://github.com/tirthajyoti/Machine-Learning-with-Python). I recommend checking it out since it contains amazing ML tutorial and practice notebooks. The following exercise is inspired by the 'Hierarchical_Clustering' notebook of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa03ff-8611-44d5-a15b-c26850ef5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "url = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/Mall_Customers.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044b3de-b1d1-492c-baad-2322fad5aa06",
   "metadata": {},
   "source": [
    "- Discover your dataset, looking at summary statistics, and histograms of income and spending score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ecc78-8162-4719-8e3f-a6b88d5237af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ba43b-b262-4878-a26f-84ed66a0f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of income and spending score\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Income\n",
    "ax[0].set_title(\"Annual income distribution\",fontsize=12)\n",
    "ax[0].set_xlabel (\"Annual income (k$)\",fontsize=10)\n",
    "sns.histplot(df['Annual Income (k$)'], ax = ax[0])\n",
    "\n",
    "# Spending score\n",
    "ax[1].set_title(\"Spending score distribution\",fontsize=12)\n",
    "ax[1].set_xlabel (\"Spending Score (1-100)\",fontsize=10)\n",
    "sns.histplot(df['Spending Score (1-100)'], color = 'green', ax = ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac8490-8610-48d7-8ba7-73c15ae89be7",
   "metadata": {},
   "source": [
    "- Do a scatter plot of income and spending score... Does your plot help you define the number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682e587-58d0-4b1e-bb9e-dba89ba046f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.title(\"Annual Income and Spending Score\")\n",
    "sns.scatterplot(data = df, x='Annual Income (k$)', y='Spending Score (1-100)', c='darkcyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4f7ca-b2a3-4a63-b9e6-478aaa993d1a",
   "metadata": {},
   "source": [
    "Dendograms can also be used to gain insights on the optimal number of clusters.\n",
    "\n",
    "- Plot a dendogram varying the linkage method. How many clusters do you think is optimal? You can apply the following technique:\n",
    "    - Look for the longest stretch of vertical line which is not crossed by any \"extended horizontal lines\" (horizontal lines grouping clusters that are extended infinitely to both directions).\n",
    "    - Take any point on that stretch of line and draw an imaginary horizontal line.\n",
    "    - Count how many vertical lines this imaginary lines crossed.\n",
    "    - That is likely to be the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18958da-7ab3-43e5-8b4b-dc5ce6ec13f3",
   "metadata": {},
   "source": [
    "*We provide below an interactive code to let the user change the linkage method. It is using the `ipywidgets` library. See the [Documentation](https://ipywidgets.readthedocs.io/en/stable/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c5361-e0db-4407-aad2-41b1b27f07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ipywidgets library\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bea1a-2719-4eb7-8102-cbe58746d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "X = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "@interact\n",
    "def interactive_dendrogram(Method = ['ward', 'single', 'complete', 'average']):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    Z = linkage(X, method = Method, metric = 'euclidean')\n",
    "    dendrogram(Z)\n",
    "    plt.title('Dendrogram - '+Method, fontsize=14)\n",
    "    plt.xlabel('Customers', fontsize=14)\n",
    "    plt.ylabel('Distance', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393473b-c5ea-4dba-bc55-f5db34651d93",
   "metadata": {},
   "source": [
    "How many clusters should we pick? Let's apply the method described above with the dendrogram using the Ward method. We notice that, for distance between about 100 and 240, the vertical lines are not crossed by extended horizontal line. If we plot an horizontal line for a distance of 200, we can count 5 clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b9052-872a-4200-90db-980db6683107",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X, method = 'ward', metric = 'euclidean')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "dendrogram(Z)\n",
    "plt.hlines(y=200,xmin=0,xmax=2000,colors='k',linestyles='--')\n",
    "plt.text(x=850,y=210,s='Horizontal line crossing 5 vertical lines',fontsize=12)\n",
    "plt.title('Dendrogram - ward', fontsize=14)\n",
    "plt.xlabel('Customers', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc10823-d8a7-4a3f-8bf8-576bc522e2b5",
   "metadata": {},
   "source": [
    "We will now implement clustering algorithms using 5 clusters representing 5 customer groups:\n",
    "- *Careful* - high income but low spenders\n",
    "- *Standard* - middle income and middle spenders\n",
    "- ***Target group*** - middle-to-high income and high spenders (should be targeted by the mall\n",
    "- *Careless* - low income but high spenders (should be avoided because of possible credit risk)\n",
    "- *Sensible* - low income and low spenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfe745-0183-4b1e-93f2-dcb3a033b4e5",
   "metadata": {},
   "source": [
    "Let's start with K-Means.\n",
    "\n",
    "- Implement a K-Means algorithm, using as features the annual income and the spending score, with 5 clusters. Also print the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e908a8b-29e1-4200-b185-b693b325f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=5, random_state=0, n_init='auto')    # KMeans model with 5 clusters)\n",
    "start_kmeans = time.time()                                            # Start recording time\n",
    "model_kmeans.fit(X)                                                   # Fit the model on X\n",
    "end_kmeans = time.time()                                              # End recording time\n",
    "k_means_time = end_kmeans-start_kmeans                                # Store the execution time\n",
    "\n",
    "print(f\"K-Means model (5 clusters) execution time: {k_means_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4250fe-f150-4633-9672-e1edf644aaab",
   "metadata": {},
   "source": [
    "- Make a scatter plot of the annual income and spending score, colored by the cluster they belong to, adding to the figure the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8b4cd-212e-4aee-84b1-e7cc2de2f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Add a column to our dataframe with the cluster labels\n",
    "df['cluster'] = model_kmeans.labels_\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Clustering of customers\")\n",
    "\n",
    "# Scatter plot colored by clusters\n",
    "sns.scatterplot(data = df, \n",
    "                x='Annual Income (k$)', \n",
    "                y='Spending Score (1-100)', \n",
    "                hue='cluster',                                                     # Color by cluster\n",
    "                palette = ['goldenrod','royalblue','green','firebrick','plum'],    # Choice of colors\n",
    "                style = 'cluster')                                                 # Style of markers by cluster\n",
    "plt.legend(loc = 'right',labels=['Sensible','Careful','Standard','Target','Careless'])  # Label legend\n",
    "\n",
    "# Area of interest (target customers) in light green\n",
    "plt.axhspan(ymin=60,ymax=100,xmin=0.4,xmax=0.96,alpha=0.3,color='lightgreen')     \n",
    "\n",
    "# Cluster centers\n",
    "plt.scatter(model_kmeans.cluster_centers_[:, 0],    # x-coordinates of cluster centroids\n",
    "              model_kmeans.cluster_centers_[:, 1],  # y-coordinates of cluster centroids\n",
    "              c=\"red\",                              # color of centroids\n",
    "              marker='x')                           # marker of centroids\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2b8d0-7c8c-40b7-819b-aeebf2da2601",
   "metadata": {},
   "source": [
    "- Implement the elbow method to determine the optimum number of cluster. Does the elbow method confirm our previous choice of 5 clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b034e3-69bb-4548-8278-bf32e7304d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "inertias = []\n",
    "nbr_clusters = range(2,11)\n",
    "\n",
    "for i in nbr_clusters:\n",
    "    km = KMeans(n_clusters=i, random_state=0, n_init='auto').fit(X)  # Create and fit model\n",
    "    inertias.append(km.inertia_)     # Store inertia\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(nbr_clusters, inertias, '-o', color='darkturquoise')\n",
    "plt.xticks(nbr_clusters)\n",
    "plt.vlines(x=5,ymin=0,ymax=200000,linestyles='--', color='k')\n",
    "plt.text(x=5.25,y=80000,s='5 clusters seem optimal', fontsize=10)\n",
    "plt.title('Elbow method for inertia')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbba1e4-524d-447b-add6-ca33fc34ad20",
   "metadata": {},
   "source": [
    "- Implement a hierarchical algorithm with 5 clusters and the linkage method of your choice. Print the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfd29c-df33-44dc-bd4b-fd48f5bd10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Hierarchical clustering with 5 clusters\n",
    "model_hc = AgglomerativeClustering(n_clusters=5,\n",
    "                                  metric='euclidean', \n",
    "                                  linkage='ward') \n",
    "start = time.time()\n",
    "model_hc.fit(X)\n",
    "end = time.time()\n",
    "hc_time=end-start\n",
    "\n",
    "print(f\"Hierarchical clustering model (5 clusters) execution time: {hc_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b4888-a2a1-4d99-97b5-1a8c6a994c3b",
   "metadata": {},
   "source": [
    "- Make a scatter plot of the annual income and spending score, colored by the cluster they belong to. How does your algorithms compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba0236-14a5-4d3f-b640-25e718d97b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Add a column to our dataframe with the cluster labels\n",
    "df['cluster hc'] = model_hc.labels_\n",
    "\n",
    "# We will do two subplot, one for K-Means, the other for hierarchical clustering\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "fig.suptitle(\"Clustering of customers\")\n",
    "\n",
    "# K-Means\n",
    "ax[0].set_title(\"K-Means\")\n",
    "sns.scatterplot(data = df, \n",
    "                x='Annual Income (k$)', \n",
    "                y='Spending Score (1-100)', \n",
    "                hue='cluster',                                                     # Color by cluster\n",
    "                palette = ['goldenrod','royalblue','green','firebrick','plum'],    # Choice of colors\n",
    "                style = 'cluster',                                                 # Style of markers by cluster\n",
    "                ax = ax[0])                                                        # Position\n",
    "ax[0].legend(loc = 'right',labels=['Sensible','Careful','Standard','Target','Careless'])  # Label legend\n",
    "\n",
    "# Hierarchical clustering\n",
    "ax[1].set_title(\"Hierarchical clustering\")\n",
    "sns.scatterplot(data = df, \n",
    "                x='Annual Income (k$)', \n",
    "                y='Spending Score (1-100)', \n",
    "                hue='cluster hc',                                                  # Color by cluster\n",
    "                palette = ['goldenrod','royalblue','green','firebrick','plum'],    # Choice of colors\n",
    "                style = 'cluster hc',                                              # Style of markers by cluster\n",
    "                ax = ax[1])                                                        # Position\n",
    "ax[1].legend(loc = 'right',labels=['Sensible','Careful','Standard','Target','Careless'])  # Label legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96688b-d966-4789-a007-7f84c09b1ffb",
   "metadata": {},
   "source": [
    "The clusters of \"Standard\" customers expand. Our \"Target\" customers are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61990081-6086-41e3-bfe6-6a4f3e5e5c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
