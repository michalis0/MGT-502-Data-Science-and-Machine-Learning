{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85baaa3a-d67f-44f4-ad59-d4122e5c85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Data Acquisition: API and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e553-36af-42c8-9522-dd090686c913",
   "metadata": {},
   "source": [
    "<img src='https://imgs.xkcd.com/comics/api.png' width=\"200\">\n",
    "\n",
    "Source: [xqcd 1481](https://xkcd.com/1481/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on exploratory data analysis throughout the use of a fun and interactive technique known as web scraping. During this laboratory, you will be provided with a broad overview of this technique as well as the major tools used in the process. You will also understand how data analysis can be conducted on real-time web data and hopefully see how this combination can be further applied to any other context. \n",
    "\n",
    "In essence, web scraping consists in harvesting the content of a web page in order to process its information for further use. In our example, web scraping is used as a fun way to extract data that we will analyze afterwards. In most cases, this technique comes hand in hand with data cleaning and data analysis. For further information on web scraping, click on the following [link](https://en.wikipedia.org/wiki/Web_scraping).\n",
    "\n",
    "**Note :** You can also refer to this [tutorial](https://www.geeksforgeeks.org/python-web-scraping-tutorial/) at any time for additional informations on this topic from a course.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [API](#API)\n",
    "- [Web scrapping with Pandas](#Web-scrapping-with-Pandas)\n",
    "- [Web scrapping libraries](#Web-scrapping-libraries)\n",
    "- [Retrieving the data](#Retrieving-the-data)\n",
    "- [Parsing the data](#Parsing-the-data)\n",
    "    - [Finding elements by class](#Finding-elements-by-class)\n",
    "    - [Finding specific elements](#Finding-specific-elements)\n",
    "- [Your turn to scrap!](#Your-turn-to-scrap!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12a56b",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2701eab",
   "metadata": {},
   "source": [
    "An [API](https://en.wikipedia.org/wiki/API), or Application Programming Interface, is a server that we can use to retrieve and send data to using code. When using an API, we make a request to a remote web server for data, and it responds to our requests. To do so, we use the same library as before, namely `requests`. \n",
    "\n",
    "To ensure we make a successful request, it is crucial to **consult the API documentation**. \n",
    "\n",
    "**Note :** You can also refer to this [tutorial](https://www.dataquest.io/blog/python-api-tutorial/) at any time for additional information on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdb5f0",
   "metadata": {},
   "source": [
    "In this lab, we will extract weather data from [OpenWeather](https://openweathermap.org/). You will need to create a free account to obtain an API key. \n",
    "\n",
    "Once you have an account, check the API documentation and request the current weather data for the city of your choice using `requests.get`. Be aware that it takes some time (about 1 hour) to activate your API key for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e403e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "# https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API key}\n",
    "lat = 45.9237\n",
    "lon = 6.8694\n",
    "key = '2672da3c3be96cac604e206bedd6ea40'  # Put your key here\n",
    "\n",
    "url_request = 'https://api.openweathermap.org/data/2.5/weather?lat='+str(lat)+'&lon='+str(lon)+'&appid='+ key\n",
    "\n",
    "weather_city = requests.get(url_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9e830",
   "metadata": {},
   "source": [
    "Check that the request was successful using `status_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ef3608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# You code here...\n",
    "\n",
    "print(weather_city.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0aad3",
   "metadata": {},
   "source": [
    "We can now look at the data. The primary format in which data is passed back and forth to APIs is [JSON](https://www.json.org/json-en.html) (JavaScript Object Notation). \n",
    "\n",
    "We can check the data obtained using the `json()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2bebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coord': {'lon': 6.8694, 'lat': 45.9237}, 'weather': [{'id': 600, 'main': 'Snow', 'description': 'light snow', 'icon': '13d'}], 'base': 'stations', 'main': {'temp': 273.05, 'feels_like': 269.79, 'temp_min': 273.05, 'temp_max': 273.05, 'pressure': 1003, 'humidity': 88, 'sea_level': 1003, 'grnd_level': 784}, 'visibility': 3021, 'wind': {'speed': 2.72, 'deg': 250, 'gust': 3.05}, 'snow': {'1h': 0.43}, 'clouds': {'all': 100}, 'dt': 1771508118, 'sys': {'country': 'FR', 'sunrise': 1771482555, 'sunset': 1771520629}, 'timezone': 3600, 'id': 3027301, 'name': 'Chamonix', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "print(weather_city.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0afb3c",
   "metadata": {},
   "source": [
    "We notice that the JSON output we received from the API looked like it contained Python dictionaries, lists, strings and integers. You can think of JSON as being a combination of these objects represented as strings. To work with JSON data, we can use the `json` package ([Documentation](https://docs.python.org/3/library/json.html), [Tutorial](https://www.w3schools.com/python/python_json.asp)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daea9c",
   "metadata": {},
   "source": [
    "Explore the weather in your city. Print the temperature, wind speed, rain and other indicators of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b511d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Chamonix, the temperature is 273.05 Kelvin and the wind speed 2.72 m/s.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "data_city = weather_city.json()\n",
    "city = data_city['name']\n",
    "temp_city = data_city['main']['temp']\n",
    "wind_city = data_city['wind']['speed']\n",
    "\n",
    "print(f'In {city}, the temperature is {temp_city} Kelvin and the wind speed {wind_city} m/s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e7f63",
   "metadata": {},
   "source": [
    "There are many other cool API out there, e.g., [NASA APIs](https://api.nasa.gov/), Google Search API [SerApi](https://serpapi.com/), Google [Earth Engine](https://earthengine.google.com/) for satellite data, [Agromonitoring](https://agromonitoring.com/) which provides satellite and weather data for agriculture, [OMDb API](https://www.omdbapi.com/) containing movie information, etc. Check them out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763984b-c9d6-43b0-ba06-fd749431aa32",
   "metadata": {},
   "source": [
    "## Web scrapping with Pandas [Optional]\n",
    "\n",
    "You are already familiar with the Pandas library and its DataFrame. Many different functions were presented to you for Dataframe manipulation. You should have noticed by now that this is a really usefull library when it comes to using tables. More over, one of the perks of using  Pandas is that you can directly scrap HTML tables from the web... Lets dive into it.\n",
    "\n",
    "The main function we are going to be using is `read_html`, it allows you the directly put the data of webpage table into a DataFrame. For more information, just click on this [link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html). \n",
    "\n",
    "For example, we want to get a list of failed banks in the link below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ca247",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\n",
    "failed_banks = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbfb4e",
   "metadata": {},
   "source": [
    "The `read_html` function scraps any table from the webpage of interest as long as it has the right format. We only want to get the first table from the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_banks[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d4f07-25df-4901-9e7b-8da09078035c",
   "metadata": {},
   "source": [
    "## Web scrapping libraries [Optional]\n",
    "\n",
    "In order to get the data from the Web with Python, we will require during the course of this lab to use the following two essential libraries:\n",
    "\n",
    "*  Requests (HTTP): retrieves the web pages (html) to parse.\n",
    "*  Beautiful Soup (HTML Parsing): parses the html.\n",
    "\n",
    "If you are working on this notebook in Colab or using JupyterLab, no specific environmental installation should be needed to work on this lab. We can directly import the needed libraries (see top of the notebook). Otherwise just install the libraries in your Anaconda/Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f616f-2ff4-489b-a218-94ad87aa51ea",
   "metadata": {},
   "source": [
    "## Retrieving the data [Optional]\n",
    "\n",
    "In order to get started with web scraping we must first make a [request](https://requests.readthedocs.io/en/master/user/quickstart/). In simple words, we will ask the server hosting the webpage we are interested in its content.\n",
    "\n",
    "Let's try it! We can use the ``requests.get`` method to retrieve information from a specified URL. \n",
    "\n",
    "We will parse through the life of [Muḥammad ibn Mūsā al-Khwārizmī](https://en.wikipedia.org/wiki/Muhammad_ibn_Musa_al-Khwarizmi). Al-Khwarizmi was a Persian polymath who is described as the father of algebra, the term originating from the title of his book *al-jabr*, meaning \"completion\" or \"rejoining\". From his name is also derived the word *algorithm*. An algorithm is a finite sequence of instructions to solve a problem - in other words it is a recipe. All the Machine Learning techniques you will see in this course are algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267bd5e-12fc-4bfb-a220-0f8531860905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "r = requests.get(\n",
    "    'https://en.wikipedia.org/wiki/Muhammad_ibn_Musa_al-Khwarizmi',\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (compatible; pandas/1.5)\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13885548-c104-4860-9d89-ec0329e4ca3d",
   "metadata": {},
   "source": [
    "We obtain a response object. We can check the status of our request using the library method ``status_code``. You can find more on the HTTP status code on this [link](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). A code of **200** means the HTTP request was successfully completed. The response header display metadata about the request itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90023f1-3268-4717-ad74-1ee16bb09ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Code\n",
    "print(f\"Response status code: {r.status_code}\\n\")\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09e884-50ac-462b-ba38-f0815fba5f9d",
   "metadata": {},
   "source": [
    "**Note :** Requesting data from a website is not always straightforward. There can be some restrictions made by the server hosting the page regarding the request origin, content or number. As such, you should always pay attention to the request policy of a website before scraping it. The standards used by websites to indicate which portions can be scraped is called [robots.txt](https://en.wikipedia.org/wiki/Robots.txt). In the case of Wikipedia, here it is:  [link](https://en.wikipedia.org/robots.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac2dbd-4227-4253-926e-f7ce357bcebb",
   "metadata": {},
   "source": [
    "Now, lets see the raw content of our request. The body of the response here will be in HTML since we are asking for a webpage. Different format such as  JSON or XML could also be imagined for web services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853e22f-c057-4517-8767-fd391f99b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d766cc1-6961-4862-a93d-5e5292e17a2c",
   "metadata": {},
   "source": [
    "## Parsing the data [Optional]\n",
    "\n",
    "Now as you can see, the HTTP response's body as it is, is hardly usable. Therefore, we rely on BeautifulSoup to parse the content for further processing. Thus, we specify that we need the html.parser. For more information, you can click [here.](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)\n",
    "\n",
    "BeautifulSoup, thanks to parsing the content, will allow us to conduct a series of different operations and commands that you will be discovering in the remaining part of this lab. This library can be very powerful and complete when it comes to parsing and manipulations. This overview is not meant to display all possible features offered by BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2a181-047d-4da0-a2f5-29dca8fbe6de",
   "metadata": {},
   "source": [
    "Before getting out any information from the HTML page, it is best to understand the structure of the page. We can do so by right-clicking on the page and select \"Inspect\", which will open the Developer Tools:\n",
    "\n",
    "<img src=\"attachment:c737ced4-bf28-4db5-9fa5-ceeb3b80f158.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcfe00-6247-4fa9-b545-7ce4c94f422d",
   "metadata": {},
   "source": [
    "Ok, let's start to parse this raw HTML code. We create a BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d3ce3-36d0-49eb-b64e-de82ad529af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865cfbe-d08e-4db0-8603-f488f890bd6a",
   "metadata": {},
   "source": [
    "We can now extract information from the page. Let's first get the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa20df-cbde-4819-adf0-ae2582f92ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d296f4b-a1c6-4a48-b560-378b5afeb368",
   "metadata": {},
   "source": [
    "Yet, this is stil in HTML format, therefore using the ``.string`` method allows for a more conventional layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2845f9d-250e-4db2-90e3-419f61fb4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b15b6f-1c39-4f62-94c8-d81e3d947f35",
   "metadata": {},
   "source": [
    "To go further with this laboratory and with Data retrieval after parsing, some HTML notions are required. In essence, you should get acquainted with concepts like **HTML tags**. Several functions and manipulations allowed by BeautifulSoup rely on the different tags (headers, divisions, paragraphs, classes, ids etc..) to retrieve the data they contain. You can find more on HTML tags [here](https://www.w3schools.com/html/html_elements.asp).\n",
    "\n",
    "**Important** : All the manipulations that are performed bellow rely on a study of the HTML body of the response. As it is specific to the website, it is fundamental to understand how to retrieve the information and how to get it from.  \n",
    "\n",
    "We will extensively use ``soup.find`` and ``soup.find_all`` to navigate the data structure, please do not hesitate to refer to the corresponding [documentation](https://https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3ee8f-13ed-42f5-9870-016b34fb71f3",
   "metadata": {},
   "source": [
    "### Finding elements by class\n",
    "\n",
    "#### Retrieving links\n",
    "\n",
    "The \"a\" tag is generally used for website links embedding (combined with ``href``). With `find_all`, we can retrieve all the links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beeb0d-45ff-44df-9bad-7ef68a6b18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "print(f'The webpage contains {len(links)} links.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb66b9-5d04-4271-b1cd-3d0c4519884d",
   "metadata": {},
   "source": [
    "We obtain a list of all the links in the webpage. Let's check some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdc45a-8867-46d2-997c-7280ffef5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links[0])\n",
    "print(links[74])\n",
    "print(links[229])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd205d-1d4d-4064-bed5-f4482c98eb2b",
   "metadata": {},
   "source": [
    "As we can see, we have different types of links, i.e., different `class`. For instance, \"mw-jump-link\" corresponds to cross-reference links, \"interlanguage-link-target\" redirects to the same article in another language, and \"mw-redirect\" to other Wikipedia pages in English.\n",
    "\n",
    "Let's extract all the links correspond to the class \"mw-redirect\". We can do so by adding a second argument to `find_all`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c56b5-905f-4763-a6c1-b9a09b33d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_wiki_redirect = soup.find_all('a', class_=\"mw-redirect\")\n",
    "print(links_wiki_redirect[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f3c05-32b3-4845-a416-ce1f817eca0f",
   "metadata": {},
   "source": [
    "We can know access the URL of the links, which is accessible with the property `href`. We can use the method `get` to extract these properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840cac3-bd12-4b4f-80fb-1487fd9a6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links_wiki_redirect[1])\n",
    "print(links_wiki_redirect[1].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50d7d0-3040-4c00-a079-c638f251518a",
   "metadata": {},
   "source": [
    "Notice the format: 'href' only contains the end of the URL. We thus need to add the beginning to obtain proper links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450d60f-c16e-46f8-9668-fbbca036d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_wiki = 'https://en.wikipedia.org'\n",
    "print(url_wiki + links_wiki_redirect[1].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbe60b-6d2e-4d77-bad8-e226570b7b09",
   "metadata": {},
   "source": [
    "Ok! Let's do it one more time. This time we'll print all the links corresponding to the images in the article. Wikipedia stores images inside `<img>` tags with the `src` attribute.\n",
    "The image URLs are typically relative (//upload.wikimedia.org/...), so you need to prepend `https:`. Wikipedia's `/static/images` URLs are not actual article content images so they \n",
    "can be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509793d-14ba-4e53-b3d3-6d0b7d8a05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = []\n",
    "for img in soup.find_all(\"img\"):\n",
    "    img_url = img.get(\"src\")\n",
    "    if img_url and img_url.startswith(\"/static/images\"):\n",
    "        continue\n",
    "    if img_url and img_url.startswith(\"//upload.wikimedia.org\"):\n",
    "        img_url = \"https:\" + img_url\n",
    "    img_urls.append(img_url)\n",
    "\n",
    "print(img_urls[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbc636-c064-4f4a-8b76-8d3116c8e4d2",
   "metadata": {},
   "source": [
    "#### Retrieving text\n",
    "\n",
    "Another common operation is to extract text from a website. If we inspect the page, we notice that the text is under the 'p' tag. We proceed as before, finding all 'p' tags in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0d298-19b6-416d-aa23-38b863f1833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find_all('p')\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625f8aa-0017-4a1d-9e97-ae14412e3b3b",
   "metadata": {},
   "source": [
    "As we can see, the format we get is not yet readable. For instance, we have various references in our paragraph that are displayed with HTML format. To extract the actual text, we can use the `text` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7d9b3-fc02-4a1f-aa85-a9cbc46aa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text[0:5]:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4ed3f-f695-490c-9fdb-8b5f2075bb05",
   "metadata": {},
   "source": [
    "Not bad! We probably need more cleaning but have achieved a readable version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86bdda-6d5c-4302-bd90-55e7420fd7ce",
   "metadata": {},
   "source": [
    "### Finding specific elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917d022-a646-40df-9fd1-1054ecd11f37",
   "metadata": {},
   "source": [
    "Above we have extracted all elements with a given tag or class. What if we want a specific element? We can generally use the `id`.\n",
    "\n",
    "For instance, let's try to extract the table of content, using `find`. By inspecting the page, we observe that the table of content falls under the `div` tag while its `id` is \"vector-toc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca56c47-a814-4b35-af55-6b02b9162b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find('div', id=\"vector-toc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b35297-b981-4885-9ad1-c47010762c25",
   "metadata": {},
   "source": [
    "As always, we need a little more processing before obtaining a readable result. The text of the table of content falls under the `class` \"vector-toc-text\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9993cd-25a4-4f3d-af61-27280b34e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = content.find_all('div', class_='vector-toc-text')\n",
    "\n",
    "for l in toc:\n",
    "    print(l.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79bf76-6fdf-4fa0-ae9d-c8db47a93c3e",
   "metadata": {},
   "source": [
    "## Your turn to scrap! [Optional]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010e414-822d-4eb3-b1f3-115e02b880eb",
   "metadata": {},
   "source": [
    "Now it's your turn to practice. We will use the [Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/index.html) (SEP), an online encyclopedia of philosophy and related fields. We will use the [Biodiversity](https://plato.stanford.edu/entries/biodiversity/) entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dc151-3c75-4362-af0c-bcad694dacd5",
   "metadata": {},
   "source": [
    "Extract the Biodiversity article of the SEP, and print the status code of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435da4a8-65c5-45fe-a2e7-d70eab5ae693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "r_biodiversity = requests.get('https://plato.stanford.edu/entries/biodiversity/')\n",
    "print(f\"Response status code: {r_biodiversity.status_code}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b479c69-6f18-47c0-b5e0-d21faf02f5f7",
   "metadata": {},
   "source": [
    "Parse the text, and print the title of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2aba55-fe3f-4a68-a34b-ee4518b94b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "biodiversity_body = r_biodiversity.text\n",
    "biodiversity_soup = BeautifulSoup(biodiversity_body, 'html.parser')\n",
    "\n",
    "print(biodiversity_soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8414e-7381-4b52-8a75-636bcab3eeec",
   "metadata": {},
   "source": [
    "Print the text of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc0e38-eb69-40b2-b68c-56c2d63387d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "content_biodiversity = biodiversity_soup.find_all('p')\n",
    "\n",
    "for t in content_biodiversity:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba4906-2d77-411a-9c39-ee31dd1a38c6",
   "metadata": {},
   "source": [
    "Print the url of the Figure A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c1528-fed5-4c7e-ae85-6fb6cf15e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "# We notice that Figure A has the idea 'figA' and the tag 'div'\n",
    "image_biodiversity = biodiversity_soup.find('div', id='figA')\n",
    "\n",
    "# The link is under 'src', but this 'src' belongs to a children tag called 'img'.\n",
    "# We extract the 'img' tag with find\n",
    "# We get the 'src' property with get\n",
    "image_src = image_biodiversity.find('img').get('src')\n",
    "\n",
    "# We got only the end of the url, we thus need to join this with the beginning of the url:\n",
    "biodiversity_url = 'https://plato.stanford.edu/entries/biodiversity/'\n",
    "\n",
    "print(biodiversity_url + image_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7587d1a-5f73-4e58-849d-c3ba19e234ba",
   "metadata": {},
   "source": [
    "Print a list of the references (i.e. the bibliography):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12d29e-5fe8-4d35-a0a9-ecf83c361167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "# We first extract the full bibliography html code, which has the tag 'div' and the property id 'bibliography'\n",
    "bibliography_biodiversity = biodiversity_soup.find('div', id='bibliography')\n",
    "\n",
    "# The references are under the 'li' tag. We extract (find) all of them\n",
    "references_biodiversity = bibliography_biodiversity.find_all('li')\n",
    "\n",
    "# We finally print the text with a for loop:\n",
    "for t in references_biodiversity:\n",
    "    print(t.text  + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
